{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook covers the modelling techniques used for getting optimal prediction results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Preparation\n",
    "\n",
    "This involves the necessary steps and processing to get both the train and test datasets in an acceptable form for the machine learnning algorithm:\n",
    "\n",
    "* Creating validation data sets\n",
    "* Selecting interested logs\n",
    "* Resolving missing values\n",
    "* Encoding categorical variables\n",
    "* Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = pd.read_csv('./data/train.csv', sep=';')\n",
    "testdata = pd.read_csv('./data/leaderboard_test_features.csv.txt', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.load('./data/penalty_matrix.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.   , 2.   , 3.5  , 3.   , 3.75 , 3.5  , 3.5  , 4.   , 4.   ,\n",
       "        2.5  , 3.875, 3.25 ],\n",
       "       [2.   , 0.   , 2.375, 2.75 , 4.   , 3.75 , 3.75 , 3.875, 4.   ,\n",
       "        3.   , 3.75 , 3.   ],\n",
       "       [3.5  , 2.375, 0.   , 2.   , 3.5  , 3.5  , 3.75 , 4.   , 4.   ,\n",
       "        2.75 , 3.25 , 3.   ],\n",
       "       [3.   , 2.75 , 2.   , 0.   , 2.5  , 2.   , 2.25 , 4.   , 4.   ,\n",
       "        3.375, 3.75 , 3.25 ],\n",
       "       [3.75 , 4.   , 3.5  , 2.5  , 0.   , 2.625, 2.875, 3.75 , 3.25 ,\n",
       "        3.   , 4.   , 3.625],\n",
       "       [3.5  , 3.75 , 3.5  , 2.   , 2.625, 0.   , 1.375, 4.   , 3.75 ,\n",
       "        3.5  , 4.   , 3.625],\n",
       "       [3.5  , 3.75 , 3.75 , 2.25 , 2.875, 1.375, 0.   , 4.   , 3.75 ,\n",
       "        3.125, 4.   , 3.75 ],\n",
       "       [4.   , 3.875, 4.   , 4.   , 3.75 , 4.   , 4.   , 0.   , 2.75 ,\n",
       "        3.75 , 3.75 , 4.   ],\n",
       "       [4.   , 4.   , 4.   , 4.   , 3.25 , 3.75 , 3.75 , 2.75 , 0.   ,\n",
       "        4.   , 4.   , 3.875],\n",
       "       [2.5  , 3.   , 2.75 , 3.375, 3.   , 3.5  , 3.125, 3.75 , 4.   ,\n",
       "        0.   , 2.5  , 3.25 ],\n",
       "       [3.875, 3.75 , 3.25 , 3.75 , 4.   , 4.   , 4.   , 3.75 , 4.   ,\n",
       "        2.5  , 0.   , 4.   ],\n",
       "       [3.25 , 3.   , 3.   , 3.25 , 3.625, 3.625, 3.75 , 4.   , 3.875,\n",
       "        3.25 , 4.   , 0.   ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(y_true, y_pred):\n",
    "\n",
    "    '''\n",
    "    custom metric used for evaluation\n",
    "    args:\n",
    "      y_true: actual prediction\n",
    "      y_pred: predictions made\n",
    "    '''\n",
    "\n",
    "    S = 0.0\n",
    "    y_true = y_true.astype(int)\n",
    "    y_pred = y_pred.astype(int)\n",
    "    for i in range(0, y_true.shape[0]):\n",
    "        S -= A[y_true[i], y_pred[i]]\n",
    "    return S/y_true.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Validation Sets\n",
    "\n",
    "Validation sets are created to properly evaluate changes made on the machine learning model. This is important to prevent overfitting the open test data since the blind test is used as the final determiner. So it is important to build ML models that will generalise better on unseen wells. Having no idea how the blind wells would come (geospatial distribution, logs presence), there was no specific guide in selecting train wells, so wells were randomly selected from the train to create two validation sets (each comprising of 10 wells). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_wells = traindata.WELL.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial total number of train wells: 98\n"
     ]
    }
   ],
   "source": [
    "print(f'Initial total number of train wells: {len(train_wells)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid1 = random.sample(list(train_wells), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QC to remove valid1 wells from train wells to prevent having same well(s) in the second validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of wells left: 88\n"
     ]
    }
   ],
   "source": [
    "train_wells = [well for well in train_wells if not well in valid1]\n",
    "print(f'Number of wells left: {len(train_wells)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of wells left: 78\n"
     ]
    }
   ],
   "source": [
    "valid2 = random.sample(list(train_wells), 10)\n",
    "\n",
    "train_wells = [well for well in train_wells if not well in valid2]\n",
    "print(f'Number of wells left: {len(train_wells)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10\n"
     ]
    }
   ],
   "source": [
    "print(len(valid1), len(valid2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "validation_wells = set(valid1 + valid2)\n",
    "print(len(validation_wells))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's proceed to getting the validation data from the train data set and dropping them to prevent any form of data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_validation_set(train, wells):\n",
    "    \n",
    "    '''\n",
    "    Function to validation sets from the full train data using well names\n",
    "    '''\n",
    "    \n",
    "    validation = pd.DataFrame(columns=list(train.columns))\n",
    "    \n",
    "    for well in wells:\n",
    "        welldata = train.loc[train.WELL == well]\n",
    "        validation = pd.concat((welldata, validation))\n",
    "        \n",
    "    return validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation1 = create_validation_set(traindata, valid1)\n",
    "validation2 = create_validation_set(traindata, valid2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total validation data\n",
    "\n",
    "validation = pd.concat((validation1, validation2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((229210, 29), (140679, 29), (88531, 29))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation.shape, validation1.shape, validation2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous train data shape: (1170511, 29)\n",
      "New train data shape: (941301, 29)\n"
     ]
    }
   ],
   "source": [
    "# dropping validation data from train data\n",
    "\n",
    "new_train = pd.concat([traindata, validation, validation]).drop_duplicates(keep=False)\n",
    "print(f'Previous train data shape: {traindata.shape}')\n",
    "print(f'New train data shape: {new_train.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of previous train data rows: 1170511\n",
      "Validation + validation rows: 1170511\n"
     ]
    }
   ],
   "source": [
    "# QC to ensure there are no data leakage\n",
    "\n",
    "previous_rows = traindata.shape[0]\n",
    "new_train_rows = new_train.shape[0]\n",
    "validation_rows = validation.shape[0]\n",
    "\n",
    "print(f'Number of previous train data rows: {previous_rows}')\n",
    "print(f'Validation + validation rows: {validation_rows+new_train_rows}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65000    577141\n",
       "30000    133966\n",
       "65030    124235\n",
       "70000     48361\n",
       "80000     29868\n",
       "99000     11482\n",
       "70032      6934\n",
       "88000      3919\n",
       "90000      3061\n",
       "74000      1428\n",
       "86000       803\n",
       "93000       103\n",
       "Name: FORCE_2020_LITHOFACIES_LITHOLOGY, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to confirm we still have all samples of the labels in the train data set\n",
    "\n",
    "new_train.FORCE_2020_LITHOFACIES_LITHOLOGY.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can proceed to other preparation procedures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logs were selected based on user's desire to use them for training. The confidence logs was dropped as this was absent in the test logs as the ML models need the same set of wells used for training in making predictions on the test wells. Other absent logs and logs with low percentage of values from the combined test data are also dropped. This has previously been visualized in the EDA notebook. A cut off of 30% may be selected as criteria for dropping the logs. Let's take a look at that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of values in test logs:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WELL         100.000000\n",
       "DEPTH_MD     100.000000\n",
       "X_LOC         99.956867\n",
       "Y_LOC         99.956867\n",
       "Z_LOC         99.956867\n",
       "GROUP        100.000000\n",
       "FORMATION     94.828418\n",
       "CALI          95.873116\n",
       "RSHA          28.582603\n",
       "RMED          99.570863\n",
       "RDEP          99.956867\n",
       "RHOB          87.601070\n",
       "GR           100.000000\n",
       "SGR            0.000000\n",
       "NPHI          76.062609\n",
       "PEF           82.978521\n",
       "DTC           99.398330\n",
       "SP            48.708932\n",
       "BS            48.955302\n",
       "ROP           49.943708\n",
       "DTS           31.596801\n",
       "DCAL           9.880397\n",
       "DRHO          81.555130\n",
       "MUDWEIGHT     14.818037\n",
       "RMIC           8.272776\n",
       "ROPA          40.786338\n",
       "RXO           21.820947\n",
       "dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Percentage of values in test logs:')\n",
    "100 - testdata.isna().sum()/testdata.shape[0]*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For better and faster processing, the train, validation and test data sets will be concatenated and processed together as we need these data sets to be in the same formats to get good predictions out of the ML model. But let's have it in mind that the RSHA, SGR, DCAL, MUDWEIGHT, RMIC and RXO will be dropped from the wells. Let's proceed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract the data sets indices that will be used for splitting the features and targets into their respective datasets after prepration is complete. We will also be extracting the train target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain = new_train.shape[0]\n",
    "ntest = testdata.shape[0]\n",
    "nvalid1 = validation1.shape[0]\n",
    "nvalid2 = validation2.shape[0]\n",
    "nvalid3 = validation.shape[0]\n",
    "\n",
    "df = pd.concat((new_train, testdata, validation1, validation2, validation)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the combined dataframe for preparation\n",
    "\n",
    "![Picture.png](images/Picture3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1536507, 29)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The procedure below is used to extract data needed for the augmentation procedure to be performed after every other preparation has been done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making a copy of the dataframes\n",
    "\n",
    "train = new_train.copy()\n",
    "test = testdata.copy()\n",
    "valid1 = validation1.copy()\n",
    "valid2 = validation2.copy()\n",
    "valid = validation.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting the data sets well names and depth values needed for augmentation\n",
    "\n",
    "train_well = train.WELL.values\n",
    "train_depth = train.DEPTH_MD.values\n",
    "\n",
    "test_well = test.WELL.values\n",
    "test_depth = test.DEPTH_MD.values\n",
    " \n",
    "valid1_well = valid1.WELL.values\n",
    "valid1_depth = valid1.DEPTH_MD.values\n",
    " \n",
    "valid2_well = valid2.WELL.values\n",
    "valid2_depth = valid2.DEPTH_MD.values\n",
    " \n",
    "valid_well = valid.WELL.values\n",
    "valid_depth = valid.DEPTH_MD.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's extract the data sets labels and prepare them for training and validation performance check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lithology = train['FORCE_2020_LITHOFACIES_LITHOLOGY']\n",
    "valid1_lithology = valid1['FORCE_2020_LITHOFACIES_LITHOLOGY']\n",
    "valid2_lithology = valid2['FORCE_2020_LITHOFACIES_LITHOLOGY']\n",
    "valid_lithology = valid['FORCE_2020_LITHOFACIES_LITHOLOGY']\n",
    " \n",
    "lithology_numbers = {30000: 0,\n",
    "                 65030: 1,\n",
    "                 65000: 2,\n",
    "                 80000: 3,\n",
    "                 74000: 4,\n",
    "                 70000: 5,\n",
    "                 70032: 6,\n",
    "                 88000: 7,\n",
    "                 86000: 8,\n",
    "                 99000: 9,\n",
    "                 90000: 10,\n",
    "                 93000: 11}\n",
    " \n",
    "lithology = lithology.map(lithology_numbers)\n",
    "valid1_lithology = valid1_lithology.map(lithology_numbers)\n",
    "valid2_lithology = valid2_lithology.map(lithology_numbers)\n",
    "valid_lithology = valid_lithology.map(lithology_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataframe shapes before dropping columns: (1536507, 29)\n",
      "shape of dataframe after dropping columns (1536507, 23)\n"
     ]
    }
   ],
   "source": [
    "print(f'dataframe shapes before dropping columns: {df.shape}')\n",
    "\n",
    "def drop_columns(data, *args):\n",
    "\n",
    "    '''\n",
    "    function used to drop columns.\n",
    "    args:: \n",
    "      data:  dataframe to be operated on\n",
    "      *args: a list of columns to be dropped from the dataframe\n",
    "\n",
    "    return: returns a dataframe with the columns dropped\n",
    "    '''\n",
    "    \n",
    "    columns = []\n",
    "    for _ in args:\n",
    "        columns.append(_)\n",
    "        \n",
    "    data = data.drop(columns, axis=1)\n",
    "        \n",
    "    return data\n",
    "\n",
    "columns_dropped = ['RSHA', 'SGR', 'DCAL', 'MUDWEIGHT', 'RMIC', 'RXO'] #columns to be dropped\n",
    "df = drop_columns(df, *columns_dropped)\n",
    "print(f'shape of dataframe after dropping columns {df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Encoding\n",
    "\n",
    "The categorical logs/columns in the data set need to be encoded for use by the ML algorithm. From the data visualization, we saw the high dimensionality of the logs (especially the FORMATION log with 69 distinct values), so label encoding will be applied instead of one hot encoding these features to prevent high dimensionality of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of dataframe after label encoding columns (1536507, 26)\n"
     ]
    }
   ],
   "source": [
    "df['GROUP_encoded'] = df['GROUP'].astype('category')\n",
    "df['GROUP_encoded'] = df['GROUP_encoded'].cat.codes \n",
    "df['FORMATION_encoded'] = df['FORMATION'].astype('category')\n",
    "df['FORMATION_encoded'] = df['FORMATION_encoded'].cat.codes\n",
    "df['WELL_encoded'] = df['WELL'].astype('category')\n",
    "df['WELL_encoded'] = df['WELL_encoded'].cat.codes\n",
    "\n",
    "print(f'shape of dataframe after label encoding columns {df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the previous columns after encoding\n",
    "\n",
    "df = df.drop(['WELL', 'GROUP', 'FORMATION'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling Missing Values\n",
    "\n",
    "Some fractions of missing values still exist in present logs, how do we resolve that? While we can use a mean of values in a window to solve this, backward or forward fill, we could also decide to fill up all missing values with a distinct value different from other values. This way, the ML algorithm used (in this case a gradient tree algorithm) can differentiate this better. From validation, this improved result better. -9999 is used, and since this is a classification problem as opposed to a regression where we predict actual lithology values, outlier effect is not observed in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(-999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DEPTH_MD                             0.0\n",
       "X_LOC                                0.0\n",
       "Y_LOC                                0.0\n",
       "Z_LOC                                0.0\n",
       "CALI                                 0.0\n",
       "RMED                                 0.0\n",
       "RDEP                                 0.0\n",
       "RHOB                                 0.0\n",
       "GR                                   0.0\n",
       "NPHI                                 0.0\n",
       "PEF                                  0.0\n",
       "DTC                                  0.0\n",
       "SP                                   0.0\n",
       "BS                                   0.0\n",
       "ROP                                  0.0\n",
       "DTS                                  0.0\n",
       "DRHO                                 0.0\n",
       "ROPA                                 0.0\n",
       "FORCE_2020_LITHOFACIES_LITHOLOGY     0.0\n",
       "FORCE_2020_LITHOFACIES_CONFIDENCE    0.0\n",
       "GROUP_encoded                        0.0\n",
       "FORMATION_encoded                    0.0\n",
       "WELL_encoded                         0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()/df.shape[0]*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've completed the majority of the preparation, let's split back our concatenated dataframe into their validation sets, train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1536507, 23)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = df.copy()   #making a copy of the preparaed dataframe to work with\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember the shape of the concatenated dataframe;\n",
    "\n",
    "![Picture.png](images/Picture3.png)\n",
    "\n",
    "using the data sets indices will be used for slicing out their corresponding features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data[:ntrain].copy()\n",
    "train.drop(['FORCE_2020_LITHOFACIES_LITHOLOGY'], axis=1, inplace=True)\n",
    "        \n",
    "test = data[ntrain:(ntest+ntrain)].copy()\n",
    "test.drop(['FORCE_2020_LITHOFACIES_LITHOLOGY'], axis=1, inplace=True)\n",
    "test = test.reset_index(drop=True)\n",
    "\n",
    "valid1 = data[(ntest+ntrain):(ntest+ntrain+nvalid1)].copy()\n",
    "valid1.drop(['FORCE_2020_LITHOFACIES_LITHOLOGY'], axis=1, inplace=True)\n",
    "valid1 = valid1.reset_index(drop=True)\n",
    "\n",
    "valid2 = data[(ntest+ntrain+nvalid1):(ntest+ntrain+nvalid1+nvalid2)].copy()\n",
    "valid2.drop(['FORCE_2020_LITHOFACIES_LITHOLOGY'], axis=1, inplace=True)\n",
    "valid2 = valid2.reset_index(drop=True)\n",
    "\n",
    "valid = data[(ntest+ntrain+nvalid1+nvalid2):].copy()\n",
    "valid.drop(['FORCE_2020_LITHOFACIES_LITHOLOGY'], axis=1, inplace=True)\n",
    "valid = valid.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((941301, 22), (136786, 22), (140679, 22), (88531, 22), (229210, 22))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking shapes of sliced data sets for QC\n",
    "\n",
    "train.shape, test.shape, valid1.shape, valid2.shape, valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "\n",
    "The data augmentation technique is extracted from the ISPL team code for the 2016 SEG ML competition : https://github.com/seg/2016-ml-contest/tree/master/ispl . The technique was based on the assumption that \"facies do not abrutly change from a given depth layer to the next one\". This was implemented by aggregating features at neighbouring depths and computing the feature spatial gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature windows concatenation function\n",
    "def augment_features_window(X, N_neig):\n",
    "    \n",
    "    # Parameters\n",
    "    N_row = X.shape[0]\n",
    "    N_feat = X.shape[1]\n",
    " \n",
    "    # Zero padding\n",
    "    X = np.vstack((np.zeros((N_neig, N_feat)), X, (np.zeros((N_neig, N_feat)))))\n",
    " \n",
    "    # Loop over windows\n",
    "    X_aug = np.zeros((N_row, N_feat*(2*N_neig+1)))\n",
    "    for r in np.arange(N_row)+N_neig:\n",
    "        this_row = []\n",
    "        for c in np.arange(-N_neig,N_neig+1):\n",
    "            this_row = np.hstack((this_row, X[r+c]))\n",
    "        X_aug[r-N_neig] = this_row\n",
    " \n",
    "    return X_aug\n",
    " \n",
    "# Feature gradient computation function\n",
    "def augment_features_gradient(X, depth):\n",
    "    \n",
    "    # Compute features gradient\n",
    "    d_diff = np.diff(depth).reshape((-1, 1))\n",
    "    d_diff[d_diff==0] = 0.001\n",
    "    X_diff = np.diff(X, axis=0)\n",
    "    X_grad = X_diff / d_diff\n",
    "        \n",
    "    # Compensate for last missing value\n",
    "    X_grad = np.concatenate((X_grad, np.zeros((1, X_grad.shape[1]))))\n",
    "    \n",
    "    return X_grad\n",
    " \n",
    "# Feature augmentation function\n",
    "def augment_features(X, well, depth, N_neig=1):\n",
    "    \n",
    "    # Augment features\n",
    "    X_aug = np.zeros((X.shape[0], X.shape[1]*(N_neig*2+2)))\n",
    "    for w in np.unique(well):\n",
    "        w_idx = np.where(well == w)[0]\n",
    "        X_aug_win = augment_features_window(X[w_idx, :], N_neig)\n",
    "        X_aug_grad = augment_features_gradient(X[w_idx, :], depth[w_idx])\n",
    "        X_aug[w_idx, :] = np.concatenate((X_aug_win, X_aug_grad), axis=1)\n",
    "    \n",
    "    return X_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of datasets before augmentation ((950452, 22), (136786, 22), (140666, 22), (79393, 22), (220059, 22))\n",
      "Shape of datasets after augmentation ((950452, 88), (136786, 88), (140666, 88), (79393, 88), (220059, 88))\n"
     ]
    }
   ],
   "source": [
    "print(f'Shape of datasets before augmentation {train.shape, test.shape, valid1.shape, valid2.shape, valid.shape}')\n",
    "\n",
    "aug_train = augment_features(train.values, train_well, train_depth)\n",
    "aug_test = augment_features(test.values, test_well, test_depth)\n",
    "aug_valid1 = augment_features(valid1.values, valid1_well, valid1_depth)\n",
    "aug_valid2 = augment_features(valid2.values, valid2_well, valid2_depth)\n",
    "aug_valid = augment_features(valid.values, valid_well, valid_depth)\n",
    "\n",
    "print(f'Shape of datasets after augmentation {aug_train.shape, aug_test.shape, aug_valid1.shape, aug_valid2.shape, aug_valid.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.716108</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.031179</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>494.528</td>\n",
       "      <td>437641.96875</td>\n",
       "      <td>6470972.5</td>\n",
       "      <td>-469.501831</td>\n",
       "      <td>19.480835</td>\n",
       "      <td>1.611410</td>\n",
       "      <td>1.798681</td>\n",
       "      <td>1.884186</td>\n",
       "      <td>80.200851</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.137015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.941753</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.026689</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>494.680</td>\n",
       "      <td>437641.96875</td>\n",
       "      <td>6470972.5</td>\n",
       "      <td>-469.653809</td>\n",
       "      <td>19.468800</td>\n",
       "      <td>1.618070</td>\n",
       "      <td>1.795641</td>\n",
       "      <td>1.889794</td>\n",
       "      <td>79.262886</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.807034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.115842</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.079409</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>494.832</td>\n",
       "      <td>437641.96875</td>\n",
       "      <td>6470972.5</td>\n",
       "      <td>-469.805786</td>\n",
       "      <td>19.468800</td>\n",
       "      <td>1.626459</td>\n",
       "      <td>1.800733</td>\n",
       "      <td>1.896523</td>\n",
       "      <td>74.821999</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.042043</td>\n",
       "      <td>0.0</td>\n",
       "      <td>115.253950</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.076305</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>494.984</td>\n",
       "      <td>437641.96875</td>\n",
       "      <td>6470972.5</td>\n",
       "      <td>-469.957794</td>\n",
       "      <td>19.459282</td>\n",
       "      <td>1.621594</td>\n",
       "      <td>1.801517</td>\n",
       "      <td>1.891913</td>\n",
       "      <td>72.878922</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.136843</td>\n",
       "      <td>0.0</td>\n",
       "      <td>117.089773</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.024252</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>495.136</td>\n",
       "      <td>437641.96875</td>\n",
       "      <td>6470972.5</td>\n",
       "      <td>-470.109772</td>\n",
       "      <td>19.453100</td>\n",
       "      <td>1.602679</td>\n",
       "      <td>1.795299</td>\n",
       "      <td>1.880034</td>\n",
       "      <td>71.729141</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.615053</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.043033</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021260</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>495.288</td>\n",
       "      <td>437641.96875</td>\n",
       "      <td>6470972.5</td>\n",
       "      <td>-470.261780</td>\n",
       "      <td>19.453100</td>\n",
       "      <td>1.585567</td>\n",
       "      <td>1.804719</td>\n",
       "      <td>1.879687</td>\n",
       "      <td>72.014420</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.647209</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.024150</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>495.440</td>\n",
       "      <td>437641.96875</td>\n",
       "      <td>6470972.5</td>\n",
       "      <td>-470.413788</td>\n",
       "      <td>19.462496</td>\n",
       "      <td>1.576569</td>\n",
       "      <td>1.805498</td>\n",
       "      <td>1.878731</td>\n",
       "      <td>72.588089</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.662001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.981283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.081083</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>495.592</td>\n",
       "      <td>437641.96875</td>\n",
       "      <td>6470972.5</td>\n",
       "      <td>-470.565796</td>\n",
       "      <td>19.468800</td>\n",
       "      <td>1.587011</td>\n",
       "      <td>1.808367</td>\n",
       "      <td>1.867837</td>\n",
       "      <td>71.283051</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.844629</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-51.167990</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.049004</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>495.744</td>\n",
       "      <td>437641.96875</td>\n",
       "      <td>6470972.5</td>\n",
       "      <td>-470.717773</td>\n",
       "      <td>19.468800</td>\n",
       "      <td>1.613674</td>\n",
       "      <td>1.815813</td>\n",
       "      <td>1.847233</td>\n",
       "      <td>69.721436</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.492102</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-137.461311</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.065673</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 88 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0             1          2           3          4         5   \\\n",
       "0    0.000       0.00000        0.0    0.000000   0.000000  0.000000   \n",
       "1  494.528  437641.96875  6470972.5 -469.501831  19.480835  1.611410   \n",
       "2  494.680  437641.96875  6470972.5 -469.653809  19.468800  1.618070   \n",
       "3  494.832  437641.96875  6470972.5 -469.805786  19.468800  1.626459   \n",
       "4  494.984  437641.96875  6470972.5 -469.957794  19.459282  1.621594   \n",
       "5  495.136  437641.96875  6470972.5 -470.109772  19.453100  1.602679   \n",
       "6  495.288  437641.96875  6470972.5 -470.261780  19.453100  1.585567   \n",
       "7  495.440  437641.96875  6470972.5 -470.413788  19.462496  1.576569   \n",
       "8  495.592  437641.96875  6470972.5 -470.565796  19.468800  1.587011   \n",
       "9  495.744  437641.96875  6470972.5 -470.717773  19.468800  1.613674   \n",
       "\n",
       "         6         7          8      9   ...        78   79          80   81  \\\n",
       "0  0.000000  0.000000   0.000000    0.0  ... -4.716108  0.0    0.000000  0.0   \n",
       "1  1.798681  1.884186  80.200851 -999.0  ...  0.137015  0.0    0.941753  0.0   \n",
       "2  1.795641  1.889794  79.262886 -999.0  ... -0.807034  0.0   34.115842  0.0   \n",
       "3  1.800733  1.896523  74.821999 -999.0  ...  2.042043  0.0  115.253950  0.0   \n",
       "4  1.801517  1.891913  72.878922 -999.0  ... -1.136843  0.0  117.089773  0.0   \n",
       "5  1.795299  1.880034  71.729141 -999.0  ... -3.615053  0.0    6.043033  0.0   \n",
       "6  1.804719  1.879687  72.014420 -999.0  ...  1.647209  0.0    0.000000  0.0   \n",
       "7  1.805498  1.878731  72.588089 -999.0  ... -9.662001  0.0   -1.981283  0.0   \n",
       "8  1.808367  1.867837  71.283051 -999.0  ...  9.844629  0.0  -51.167990  0.0   \n",
       "9  1.815813  1.847233  69.721436 -999.0  ...  2.492102  0.0 -137.461311  0.0   \n",
       "\n",
       "         82   83   84   85   86   87  \n",
       "0  0.031179  0.0  0.0  0.0  0.0  0.0  \n",
       "1 -0.026689  0.0  0.0  0.0  0.0  0.0  \n",
       "2 -0.079409  0.0  0.0  0.0  0.0  0.0  \n",
       "3 -0.076305  0.0  0.0  0.0  0.0  0.0  \n",
       "4 -0.024252  0.0  0.0  0.0  0.0  0.0  \n",
       "5  0.021260  0.0  0.0  0.0  0.0  0.0  \n",
       "6 -0.024150  0.0  0.0  0.0  0.0  0.0  \n",
       "7 -0.081083  0.0  0.0  0.0  0.0  0.0  \n",
       "8 -0.049004  0.0  0.0  0.0  0.0  0.0  \n",
       "9  0.065673  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[10 rows x 88 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(aug_train).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DEPTH_MD</th>\n",
       "      <th>X_LOC</th>\n",
       "      <th>Y_LOC</th>\n",
       "      <th>Z_LOC</th>\n",
       "      <th>CALI</th>\n",
       "      <th>RMED</th>\n",
       "      <th>RDEP</th>\n",
       "      <th>RHOB</th>\n",
       "      <th>GR</th>\n",
       "      <th>NPHI</th>\n",
       "      <th>...</th>\n",
       "      <th>SP</th>\n",
       "      <th>BS</th>\n",
       "      <th>ROP</th>\n",
       "      <th>DTS</th>\n",
       "      <th>DRHO</th>\n",
       "      <th>ROPA</th>\n",
       "      <th>FORCE_2020_LITHOFACIES_CONFIDENCE</th>\n",
       "      <th>GROUP_encoded</th>\n",
       "      <th>FORMATION_encoded</th>\n",
       "      <th>WELL_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>494.528</td>\n",
       "      <td>437641.96875</td>\n",
       "      <td>6470972.5</td>\n",
       "      <td>-469.501831</td>\n",
       "      <td>19.480835</td>\n",
       "      <td>1.611410</td>\n",
       "      <td>1.798681</td>\n",
       "      <td>1.884186</td>\n",
       "      <td>80.200851</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>...</td>\n",
       "      <td>24.612379</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>34.636410</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-0.574928</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>494.680</td>\n",
       "      <td>437641.96875</td>\n",
       "      <td>6470972.5</td>\n",
       "      <td>-469.653809</td>\n",
       "      <td>19.468800</td>\n",
       "      <td>1.618070</td>\n",
       "      <td>1.795641</td>\n",
       "      <td>1.889794</td>\n",
       "      <td>79.262886</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>...</td>\n",
       "      <td>23.895531</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>34.636410</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-0.570188</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>494.832</td>\n",
       "      <td>437641.96875</td>\n",
       "      <td>6470972.5</td>\n",
       "      <td>-469.805786</td>\n",
       "      <td>19.468800</td>\n",
       "      <td>1.626459</td>\n",
       "      <td>1.800733</td>\n",
       "      <td>1.896523</td>\n",
       "      <td>74.821999</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>...</td>\n",
       "      <td>23.916357</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>34.779556</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-0.574245</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>494.984</td>\n",
       "      <td>437641.96875</td>\n",
       "      <td>6470972.5</td>\n",
       "      <td>-469.957794</td>\n",
       "      <td>19.459282</td>\n",
       "      <td>1.621594</td>\n",
       "      <td>1.801517</td>\n",
       "      <td>1.891913</td>\n",
       "      <td>72.878922</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>...</td>\n",
       "      <td>23.793688</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>39.965164</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-0.586315</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>495.136</td>\n",
       "      <td>437641.96875</td>\n",
       "      <td>6470972.5</td>\n",
       "      <td>-470.109772</td>\n",
       "      <td>19.453100</td>\n",
       "      <td>1.602679</td>\n",
       "      <td>1.795299</td>\n",
       "      <td>1.880034</td>\n",
       "      <td>71.729141</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>...</td>\n",
       "      <td>24.104078</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>57.483765</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-0.597914</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>495.288</td>\n",
       "      <td>437641.96875</td>\n",
       "      <td>6470972.5</td>\n",
       "      <td>-470.261780</td>\n",
       "      <td>19.453100</td>\n",
       "      <td>1.585567</td>\n",
       "      <td>1.804719</td>\n",
       "      <td>1.879687</td>\n",
       "      <td>72.014420</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>...</td>\n",
       "      <td>23.931278</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>75.281410</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-0.601600</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>495.440</td>\n",
       "      <td>437641.96875</td>\n",
       "      <td>6470972.5</td>\n",
       "      <td>-470.413788</td>\n",
       "      <td>19.462496</td>\n",
       "      <td>1.576569</td>\n",
       "      <td>1.805498</td>\n",
       "      <td>1.878731</td>\n",
       "      <td>72.588089</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>...</td>\n",
       "      <td>23.381790</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>76.199951</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-0.598369</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>495.592</td>\n",
       "      <td>437641.96875</td>\n",
       "      <td>6470972.5</td>\n",
       "      <td>-470.565796</td>\n",
       "      <td>19.468800</td>\n",
       "      <td>1.587011</td>\n",
       "      <td>1.808367</td>\n",
       "      <td>1.867837</td>\n",
       "      <td>71.283051</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>...</td>\n",
       "      <td>23.632166</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>76.199951</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-0.602039</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>495.744</td>\n",
       "      <td>437641.96875</td>\n",
       "      <td>6470972.5</td>\n",
       "      <td>-470.717773</td>\n",
       "      <td>19.468800</td>\n",
       "      <td>1.613674</td>\n",
       "      <td>1.815813</td>\n",
       "      <td>1.847233</td>\n",
       "      <td>69.721436</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>...</td>\n",
       "      <td>22.163542</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>75.898796</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-0.614364</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>495.896</td>\n",
       "      <td>437641.96875</td>\n",
       "      <td>6470972.5</td>\n",
       "      <td>-470.869781</td>\n",
       "      <td>19.468800</td>\n",
       "      <td>1.634622</td>\n",
       "      <td>1.813916</td>\n",
       "      <td>1.836309</td>\n",
       "      <td>66.677727</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>...</td>\n",
       "      <td>23.659925</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>68.121262</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-0.621813</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   DEPTH_MD         X_LOC      Y_LOC       Z_LOC       CALI      RMED  \\\n",
       "0   494.528  437641.96875  6470972.5 -469.501831  19.480835  1.611410   \n",
       "1   494.680  437641.96875  6470972.5 -469.653809  19.468800  1.618070   \n",
       "2   494.832  437641.96875  6470972.5 -469.805786  19.468800  1.626459   \n",
       "3   494.984  437641.96875  6470972.5 -469.957794  19.459282  1.621594   \n",
       "4   495.136  437641.96875  6470972.5 -470.109772  19.453100  1.602679   \n",
       "5   495.288  437641.96875  6470972.5 -470.261780  19.453100  1.585567   \n",
       "6   495.440  437641.96875  6470972.5 -470.413788  19.462496  1.576569   \n",
       "7   495.592  437641.96875  6470972.5 -470.565796  19.468800  1.587011   \n",
       "8   495.744  437641.96875  6470972.5 -470.717773  19.468800  1.613674   \n",
       "9   495.896  437641.96875  6470972.5 -470.869781  19.468800  1.634622   \n",
       "\n",
       "       RDEP      RHOB         GR   NPHI  ...         SP     BS        ROP  \\\n",
       "0  1.798681  1.884186  80.200851 -999.0  ...  24.612379 -999.0  34.636410   \n",
       "1  1.795641  1.889794  79.262886 -999.0  ...  23.895531 -999.0  34.636410   \n",
       "2  1.800733  1.896523  74.821999 -999.0  ...  23.916357 -999.0  34.779556   \n",
       "3  1.801517  1.891913  72.878922 -999.0  ...  23.793688 -999.0  39.965164   \n",
       "4  1.795299  1.880034  71.729141 -999.0  ...  24.104078 -999.0  57.483765   \n",
       "5  1.804719  1.879687  72.014420 -999.0  ...  23.931278 -999.0  75.281410   \n",
       "6  1.805498  1.878731  72.588089 -999.0  ...  23.381790 -999.0  76.199951   \n",
       "7  1.808367  1.867837  71.283051 -999.0  ...  23.632166 -999.0  76.199951   \n",
       "8  1.815813  1.847233  69.721436 -999.0  ...  22.163542 -999.0  75.898796   \n",
       "9  1.813916  1.836309  66.677727 -999.0  ...  23.659925 -999.0  68.121262   \n",
       "\n",
       "     DTS      DRHO   ROPA  FORCE_2020_LITHOFACIES_CONFIDENCE  GROUP_encoded  \\\n",
       "0 -999.0 -0.574928 -999.0                                1.0              6   \n",
       "1 -999.0 -0.570188 -999.0                                1.0              6   \n",
       "2 -999.0 -0.574245 -999.0                                1.0              6   \n",
       "3 -999.0 -0.586315 -999.0                                1.0              6   \n",
       "4 -999.0 -0.597914 -999.0                                1.0              6   \n",
       "5 -999.0 -0.601600 -999.0                                1.0              6   \n",
       "6 -999.0 -0.598369 -999.0                                1.0              6   \n",
       "7 -999.0 -0.602039 -999.0                                1.0              6   \n",
       "8 -999.0 -0.614364 -999.0                                1.0              6   \n",
       "9 -999.0 -0.621813 -999.0                                1.0              6   \n",
       "\n",
       "   FORMATION_encoded  WELL_encoded  \n",
       "0                 -1             0  \n",
       "1                 -1             0  \n",
       "2                 -1             0  \n",
       "3                 -1             0  \n",
       "4                 -1             0  \n",
       "5                 -1             0  \n",
       "6                 -1             0  \n",
       "7                 -1             0  \n",
       "8                 -1             0  \n",
       "9                 -1             0  \n",
       "\n",
       "[10 rows x 22 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "\n",
    "The choice of algorithm for this tutorial workflow is xgboost. Why? Performance on previously done validation was better, and also at a faster compute speed than catboost. Random forest is also a great algorithm to try. Let's implement our xgboost tree. This will be done in a 10 fold cross validation technique. This is done to get a better performance and a confident result that is not due to randomness. We will be using StratifiedKFold function from sklearn. Let's look at that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_evaluation(pred, true):\n",
    "\n",
    "  '''\n",
    "\n",
    "  function to show model performance and evaluation\n",
    "  args:\n",
    "    pred: predicted value(a list)\n",
    "    true: actual values (a list)\n",
    "\n",
    "  prints the custom metric performance, accuracy and F1 score of predictions\n",
    "\n",
    "  '''\n",
    "\n",
    "  print(f'Default score: {score(true.values, pred)}')\n",
    "  print(f'Accuracy is: {accuracy_score(true, pred)}')\n",
    "  print(f'F1 is: {f1_score(pred, true.values, average=\"weighted\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 10\n",
    "\n",
    "kf = StratifiedKFold(n_splits=split, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBClassifier(n_estimators=1, max_depth=10, booster='gbtree',\n",
    "                          objective='softprob', learning_rate=0.1, random_state=0,\n",
    "                          subsample=0.9, colsample_bytree=0.9, tree_method='gpu_hist',\n",
    "                          eval_metric='mlogloss', reg_lambda=1500, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = np.zeros((len(test), 12))\n",
    "valid1_pred = np.zeros((len(valid1), 12))\n",
    "valid2_pred = np.zeros((len(valid2), 12))\n",
    "valid_pred = np.zeros((len(valid), 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:2.16277\n",
      "Default score: -0.9560968588062184\n",
      "Accuracy is: 0.680819454218469\n",
      "F1 is: 0.6929869606281023\n",
      "None\n",
      "Default score: -0.9098521986648744\n",
      "Accuracy is: 0.685296675740701\n",
      "F1 is: 0.7303326055476616\n",
      "None\n",
      "Default score: -0.9382351337201693\n",
      "Accuracy is: 0.6825487544173465\n",
      "F1 is: 0.7077298254188961\n",
      "None\n",
      "----------------------- FOLD 1 ---------------------\n",
      "[0]\tvalidation_0-mlogloss:2.16385\n",
      "Default score: -0.9484064785788924\n",
      "Accuracy is: 0.6829732938107322\n",
      "F1 is: 0.6947328307151964\n",
      "None\n",
      "Default score: -0.8603003467711875\n",
      "Accuracy is: 0.6816934181247247\n",
      "F1 is: 0.715754022472603\n",
      "None\n",
      "Default score: -0.9143760089001353\n",
      "Accuracy is: 0.682478949435016\n",
      "F1 is: 0.702804197086393\n",
      "None\n",
      "----------------------- FOLD 2 ---------------------\n",
      "[0]\tvalidation_0-mlogloss:2.16325\n",
      "Default score: -0.9599984716979791\n",
      "Accuracy is: 0.679404886301438\n",
      "F1 is: 0.6895443225843482\n",
      "None\n",
      "Default score: -0.8537517931571992\n",
      "Accuracy is: 0.6857484948775006\n",
      "F1 is: 0.7155316946757434\n",
      "None\n",
      "Default score: -0.9189613236769775\n",
      "Accuracy is: 0.6818550674054361\n",
      "F1 is: 0.699270935965995\n",
      "None\n",
      "----------------------- FOLD 3 ---------------------\n",
      "[0]\tvalidation_0-mlogloss:2.16318\n",
      "Default score: -1.0418186083210714\n",
      "Accuracy is: 0.6558406016534095\n",
      "F1 is: 0.6628266616895732\n",
      "None\n",
      "Default score: -0.8952626763506568\n",
      "Accuracy is: 0.6717308061582948\n",
      "F1 is: 0.7015895737306329\n",
      "None\n",
      "Default score: -0.985212250774399\n",
      "Accuracy is: 0.6619780986867938\n",
      "F1 is: 0.6772049376070898\n",
      "None\n",
      "----------------------- FOLD 4 ---------------------\n",
      "[0]\tvalidation_0-mlogloss:2.16279\n",
      "Default score: -1.0448663268860314\n",
      "Accuracy is: 0.6548169947184725\n",
      "F1 is: 0.6609239614971737\n",
      "None\n",
      "Default score: -0.900995131648801\n",
      "Accuracy is: 0.6869458155900193\n",
      "F1 is: 0.7272148993684288\n",
      "None\n",
      "Default score: -0.9892969329435889\n",
      "Accuracy is: 0.6672265607957768\n",
      "F1 is: 0.684950720810129\n",
      "None\n",
      "----------------------- FOLD 5 ---------------------\n",
      "[0]\tvalidation_0-mlogloss:2.16290\n",
      "Default score: -0.9960584024623433\n",
      "Accuracy is: 0.6623234455746771\n",
      "F1 is: 0.6680558247177665\n",
      "None\n",
      "Default score: -0.949519095006269\n",
      "Accuracy is: 0.6421818346116049\n",
      "F1 is: 0.6870509533931841\n",
      "None\n",
      "Default score: -0.9780828716024607\n",
      "Accuracy is: 0.6545438680685834\n",
      "F1 is: 0.6748228123706523\n",
      "None\n",
      "----------------------- FOLD 6 ---------------------\n",
      "[0]\tvalidation_0-mlogloss:2.16251\n",
      "Default score: -1.0395385949573142\n",
      "Accuracy is: 0.6573973372002929\n",
      "F1 is: 0.6650124828957239\n",
      "None\n",
      "Default score: -0.8478047237690752\n",
      "Accuracy is: 0.6894534117992568\n",
      "F1 is: 0.7195987033572584\n",
      "None\n",
      "Default score: -0.9654825269403604\n",
      "Accuracy is: 0.6697788054622399\n",
      "F1 is: 0.6854258931538469\n",
      "None\n",
      "----------------------- FOLD 7 ---------------------\n",
      "[0]\tvalidation_0-mlogloss:2.16343\n",
      "Default score: -1.0312066833002793\n",
      "Accuracy is: 0.6591388906659843\n",
      "F1 is: 0.6657335172254257\n",
      "None\n",
      "Default score: -0.9093890840496549\n",
      "Accuracy is: 0.682405033265184\n",
      "F1 is: 0.7272932292774377\n",
      "None\n",
      "Default score: -0.9841553597137995\n",
      "Accuracy is: 0.6681252999432834\n",
      "F1 is: 0.6887402772301486\n",
      "None\n",
      "----------------------- FOLD 8 ---------------------\n",
      "[0]\tvalidation_0-mlogloss:2.16349\n",
      "Default score: -0.9549337498844888\n",
      "Accuracy is: 0.6820705293611697\n",
      "F1 is: 0.6952017774397404\n",
      "None\n",
      "Default score: -0.9276270458935288\n",
      "Accuracy is: 0.6778755464187686\n",
      "F1 is: 0.7092434327598068\n",
      "None\n",
      "Default score: -0.9443866977880546\n",
      "Accuracy is: 0.6804502421360324\n",
      "F1 is: 0.7007668210245062\n",
      "None\n",
      "----------------------- FOLD 9 ---------------------\n",
      "[0]\tvalidation_0-mlogloss:2.16389\n",
      "Default score: -0.9795340811350663\n",
      "Accuracy is: 0.6660979961472572\n",
      "F1 is: 0.6715118287065103\n",
      "None\n",
      "Default score: -0.8842382894127481\n",
      "Accuracy is: 0.6774011363251291\n",
      "F1 is: 0.7037887780363443\n",
      "None\n",
      "Default score: -0.9427266480520047\n",
      "Accuracy is: 0.670463766851359\n",
      "F1 is: 0.6844870225272921\n",
      "None\n",
      "----------------------- FOLD 10 ---------------------\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "for (train_index, test_index) in kf.split(train, lithology):\n",
    "    X_train,X_test = train.iloc[train_index], train.iloc[test_index]\n",
    "    Y_train,Y_test = lithology.iloc[train_index], lithology.iloc[test_index]\n",
    "    \n",
    "        \n",
    "    model.fit(X_train, Y_train, early_stopping_rounds=100, eval_set=[(X_test, Y_test)], verbose=1)\n",
    "    \n",
    "    prediction1 = model.predict(valid1)\n",
    "    prediction2 = model.predict(valid2)\n",
    "    prediction = model.predict(valid)\n",
    "    print(show_evaluation(prediction1, valid1_lithology))\n",
    "    print(show_evaluation(prediction2, valid2_lithology))\n",
    "    print(show_evaluation(prediction, valid_lithology))\n",
    " \n",
    "    print(f'----------------------- FOLD {i} ---------------------')\n",
    "    i+=1\n",
    "    \n",
    "    valid1_pred += model.predict_proba(valid1)\n",
    "    valid2_pred += model.predict_proba(valid2)\n",
    "    valid_pred += model.predict_proba(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid1_pred = pd.DataFrame(valid1_pred/split)\n",
    "valid2_pred = pd.DataFrame(valid2_pred/split)\n",
    "valid_pred = pd.DataFrame(valid_pred/split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid1_pred = valid1_pred.idxmax(axis=1)\n",
    "valid2_pred = valid2_pred.idxmax(axis=1)\n",
    "valid_pred = valid_pred.idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default score: -0.9571631160301111\n",
      "Accuracy is: 0.6801939166471186\n",
      "F1 is: 0.6918461589943973\n"
     ]
    }
   ],
   "source": [
    "show_evaluation(valid1_pred, valid1_lithology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default score: -0.8554701178118399\n",
      "Accuracy is: 0.6841784233771221\n",
      "F1 is: 0.7175537752878195\n"
     ]
    }
   ],
   "source": [
    "show_evaluation(valid2_pred, valid2_lithology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default score: -0.9178847999650975\n",
      "Accuracy is: 0.6817329086863575\n",
      "F1 is: 0.7019928305401797\n"
     ]
    }
   ],
   "source": [
    "show_evaluation(valid_pred, valid_lithology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
