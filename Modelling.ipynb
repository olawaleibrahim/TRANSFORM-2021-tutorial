{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook covers the modelling techniques used for getting optimal prediction results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Preparation\n",
    "\n",
    "This involves the necessary steps and processing to get both the train and test datasets in an acceptable form for the machine learnning algorithm:\n",
    "\n",
    "* Creating validation data sets\n",
    "* Selecting interested logs\n",
    "* Resolving missing values\n",
    "* Encoding categorical variables\n",
    "* Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = pd.read_csv('./data/train.csv', sep=';')\n",
    "testdata = pd.read_csv('./data/leaderboard_test_features.csv.txt', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.load('./data/penalty_matrix.npy')   # penalty matrix used for scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.   , 2.   , 3.5  , 3.   , 3.75 , 3.5  , 3.5  , 4.   , 4.   ,\n",
       "        2.5  , 3.875, 3.25 ],\n",
       "       [2.   , 0.   , 2.375, 2.75 , 4.   , 3.75 , 3.75 , 3.875, 4.   ,\n",
       "        3.   , 3.75 , 3.   ],\n",
       "       [3.5  , 2.375, 0.   , 2.   , 3.5  , 3.5  , 3.75 , 4.   , 4.   ,\n",
       "        2.75 , 3.25 , 3.   ],\n",
       "       [3.   , 2.75 , 2.   , 0.   , 2.5  , 2.   , 2.25 , 4.   , 4.   ,\n",
       "        3.375, 3.75 , 3.25 ],\n",
       "       [3.75 , 4.   , 3.5  , 2.5  , 0.   , 2.625, 2.875, 3.75 , 3.25 ,\n",
       "        3.   , 4.   , 3.625],\n",
       "       [3.5  , 3.75 , 3.5  , 2.   , 2.625, 0.   , 1.375, 4.   , 3.75 ,\n",
       "        3.5  , 4.   , 3.625],\n",
       "       [3.5  , 3.75 , 3.75 , 2.25 , 2.875, 1.375, 0.   , 4.   , 3.75 ,\n",
       "        3.125, 4.   , 3.75 ],\n",
       "       [4.   , 3.875, 4.   , 4.   , 3.75 , 4.   , 4.   , 0.   , 2.75 ,\n",
       "        3.75 , 3.75 , 4.   ],\n",
       "       [4.   , 4.   , 4.   , 4.   , 3.25 , 3.75 , 3.75 , 2.75 , 0.   ,\n",
       "        4.   , 4.   , 3.875],\n",
       "       [2.5  , 3.   , 2.75 , 3.375, 3.   , 3.5  , 3.125, 3.75 , 4.   ,\n",
       "        0.   , 2.5  , 3.25 ],\n",
       "       [3.875, 3.75 , 3.25 , 3.75 , 4.   , 4.   , 4.   , 3.75 , 4.   ,\n",
       "        2.5  , 0.   , 4.   ],\n",
       "       [3.25 , 3.   , 3.   , 3.25 , 3.625, 3.625, 3.75 , 4.   , 3.875,\n",
       "        3.25 , 4.   , 0.   ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scoring matrix for petrophysical interpretation\n",
    "\n",
    "![Penalty.png](images/penalty_matrix.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(y_true, y_pred):\n",
    "\n",
    "    '''\n",
    "    custom metric used for evaluation\n",
    "    args:\n",
    "      y_true: actual prediction\n",
    "      y_pred: predictions made\n",
    "    '''\n",
    "\n",
    "    S = 0.0\n",
    "    y_true = y_true.astype(int)\n",
    "    y_pred = y_pred.astype(int)\n",
    "    for i in range(0, y_true.shape[0]):\n",
    "        S -= A[y_true[i], y_pred[i]]\n",
    "    return S/y_true.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Validation Sets\n",
    "\n",
    "Validation sets are created to properly evaluate changes made on the machine learning model. This is important to prevent overfitting the open test data since the blind test is used as the final determiner. So it is important to build ML models that will generalise better on unseen wells. Having no idea how the blind wells would come (geospatial distribution, logs presence), there was no specific guide in selecting train wells, so wells were randomly selected from the train to create two validation sets (each comprising of 10 wells). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_wells = traindata.WELL.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial total number of train wells: 98\n"
     ]
    }
   ],
   "source": [
    "print(f'Initial total number of train wells: {len(train_wells)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid1 = random.sample(list(train_wells), 10)   #randomly sample 10 well names from train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['31/6-5',\n",
       " '34/7-13',\n",
       " '30/3-5 S',\n",
       " '34/10-19',\n",
       " '34/11-2 S',\n",
       " '15/9-15',\n",
       " '35/3-7 S',\n",
       " '31/4-5',\n",
       " '35/11-15 S',\n",
       " '31/3-4']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QC to remove valid1 wells from train wells to prevent having same well(s) in the second validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of wells left: 68\n"
     ]
    }
   ],
   "source": [
    "train_wells = [well for well in train_wells if not well in valid1]\n",
    "print(f'Number of wells left: {len(train_wells)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of wells left: 58\n"
     ]
    }
   ],
   "source": [
    "valid2 = random.sample(list(train_wells), 10)\n",
    "\n",
    "train_wells = [well for well in train_wells if not well in valid2]\n",
    "print(f'Number of wells left: {len(train_wells)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10\n"
     ]
    }
   ],
   "source": [
    "print(len(valid1), len(valid2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "validation_wells = set(valid1 + valid2)\n",
    "print(len(validation_wells))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's proceed to getting the validation data from the train data set and dropping them to prevent any form of data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_validation_set(train, wells):\n",
    "    \n",
    "    '''\n",
    "    Function to validation sets from the full train data using well names\n",
    "    '''\n",
    "    \n",
    "    validation = pd.DataFrame(columns=list(train.columns))\n",
    "    \n",
    "    for well in wells:\n",
    "        welldata = train.loc[train.WELL == well]\n",
    "        validation = pd.concat((welldata, validation))\n",
    "        \n",
    "    return validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using function to get data for validation wells\n",
    "\n",
    "validation1 = create_validation_set(traindata, valid1)\n",
    "validation2 = create_validation_set(traindata, valid2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WELL</th>\n",
       "      <th>DEPTH_MD</th>\n",
       "      <th>X_LOC</th>\n",
       "      <th>Y_LOC</th>\n",
       "      <th>Z_LOC</th>\n",
       "      <th>GROUP</th>\n",
       "      <th>FORMATION</th>\n",
       "      <th>CALI</th>\n",
       "      <th>RSHA</th>\n",
       "      <th>RMED</th>\n",
       "      <th>...</th>\n",
       "      <th>ROP</th>\n",
       "      <th>DTS</th>\n",
       "      <th>DCAL</th>\n",
       "      <th>DRHO</th>\n",
       "      <th>MUDWEIGHT</th>\n",
       "      <th>RMIC</th>\n",
       "      <th>ROPA</th>\n",
       "      <th>RXO</th>\n",
       "      <th>FORCE_2020_LITHOFACIES_LITHOLOGY</th>\n",
       "      <th>FORCE_2020_LITHOFACIES_CONFIDENCE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>924195</th>\n",
       "      <td>34/8-1</td>\n",
       "      <td>891.137023</td>\n",
       "      <td>469664.71875</td>\n",
       "      <td>6803719.0</td>\n",
       "      <td>-868.119202</td>\n",
       "      <td>NORDLAND GP.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.342517</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.722545</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924196</th>\n",
       "      <td>34/8-1</td>\n",
       "      <td>891.289023</td>\n",
       "      <td>469664.71875</td>\n",
       "      <td>6803719.0</td>\n",
       "      <td>-868.271179</td>\n",
       "      <td>NORDLAND GP.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.310889</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.698928</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924197</th>\n",
       "      <td>34/8-1</td>\n",
       "      <td>891.441023</td>\n",
       "      <td>469664.71875</td>\n",
       "      <td>6803719.0</td>\n",
       "      <td>-868.423218</td>\n",
       "      <td>NORDLAND GP.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.320935</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.684372</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924198</th>\n",
       "      <td>34/8-1</td>\n",
       "      <td>891.593023</td>\n",
       "      <td>469664.71875</td>\n",
       "      <td>6803719.0</td>\n",
       "      <td>-868.575195</td>\n",
       "      <td>NORDLAND GP.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.275842</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.661161</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924199</th>\n",
       "      <td>34/8-1</td>\n",
       "      <td>891.745023</td>\n",
       "      <td>469664.71875</td>\n",
       "      <td>6803719.0</td>\n",
       "      <td>-868.727173</td>\n",
       "      <td>NORDLAND GP.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.223282</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.662985</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152225</th>\n",
       "      <td>36/7-3</td>\n",
       "      <td>2947.576000</td>\n",
       "      <td>556082.31250</td>\n",
       "      <td>6810860.0</td>\n",
       "      <td>-2922.401367</td>\n",
       "      <td>VIKING GP.</td>\n",
       "      <td>Heather Fm.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.895206</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.90472</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65030</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152226</th>\n",
       "      <td>36/7-3</td>\n",
       "      <td>2947.728000</td>\n",
       "      <td>556082.31250</td>\n",
       "      <td>6810860.0</td>\n",
       "      <td>-2922.553467</td>\n",
       "      <td>VIKING GP.</td>\n",
       "      <td>Heather Fm.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.63416</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65030</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152227</th>\n",
       "      <td>36/7-3</td>\n",
       "      <td>2947.880000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>VIKING GP.</td>\n",
       "      <td>Heather Fm.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.36360</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65030</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152228</th>\n",
       "      <td>36/7-3</td>\n",
       "      <td>2948.032000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>VIKING GP.</td>\n",
       "      <td>Heather Fm.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65030</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152229</th>\n",
       "      <td>36/7-3</td>\n",
       "      <td>2948.184000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>VIKING GP.</td>\n",
       "      <td>Heather Fm.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65030</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>132671 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           WELL     DEPTH_MD         X_LOC      Y_LOC        Z_LOC  \\\n",
       "924195   34/8-1   891.137023  469664.71875  6803719.0  -868.119202   \n",
       "924196   34/8-1   891.289023  469664.71875  6803719.0  -868.271179   \n",
       "924197   34/8-1   891.441023  469664.71875  6803719.0  -868.423218   \n",
       "924198   34/8-1   891.593023  469664.71875  6803719.0  -868.575195   \n",
       "924199   34/8-1   891.745023  469664.71875  6803719.0  -868.727173   \n",
       "...         ...          ...           ...        ...          ...   \n",
       "1152225  36/7-3  2947.576000  556082.31250  6810860.0 -2922.401367   \n",
       "1152226  36/7-3  2947.728000  556082.31250  6810860.0 -2922.553467   \n",
       "1152227  36/7-3  2947.880000           NaN        NaN          NaN   \n",
       "1152228  36/7-3  2948.032000           NaN        NaN          NaN   \n",
       "1152229  36/7-3  2948.184000           NaN        NaN          NaN   \n",
       "\n",
       "                GROUP    FORMATION  CALI  RSHA       RMED  ...  ROP  DTS  \\\n",
       "924195   NORDLAND GP.          NaN   NaN   NaN   2.342517  ...  NaN  NaN   \n",
       "924196   NORDLAND GP.          NaN   NaN   NaN   2.310889  ...  NaN  NaN   \n",
       "924197   NORDLAND GP.          NaN   NaN   NaN   2.320935  ...  NaN  NaN   \n",
       "924198   NORDLAND GP.          NaN   NaN   NaN   2.275842  ...  NaN  NaN   \n",
       "924199   NORDLAND GP.          NaN   NaN   NaN   2.223282  ...  NaN  NaN   \n",
       "...               ...          ...   ...   ...        ...  ...  ...  ...   \n",
       "1152225    VIKING GP.  Heather Fm.   NaN   NaN  21.895206  ...  NaN  NaN   \n",
       "1152226    VIKING GP.  Heather Fm.   NaN   NaN        NaN  ...  NaN  NaN   \n",
       "1152227    VIKING GP.  Heather Fm.   NaN   NaN        NaN  ...  NaN  NaN   \n",
       "1152228    VIKING GP.  Heather Fm.   NaN   NaN        NaN  ...  NaN  NaN   \n",
       "1152229    VIKING GP.  Heather Fm.   NaN   NaN        NaN  ...  NaN  NaN   \n",
       "\n",
       "         DCAL      DRHO  MUDWEIGHT  RMIC     ROPA  RXO  \\\n",
       "924195    NaN -0.722545        NaN   NaN      NaN  NaN   \n",
       "924196    NaN -0.698928        NaN   NaN      NaN  NaN   \n",
       "924197    NaN -0.684372        NaN   NaN      NaN  NaN   \n",
       "924198    NaN -0.661161        NaN   NaN      NaN  NaN   \n",
       "924199    NaN -0.662985        NaN   NaN      NaN  NaN   \n",
       "...       ...       ...        ...   ...      ...  ...   \n",
       "1152225   NaN       NaN        NaN   NaN  9.90472  NaN   \n",
       "1152226   NaN       NaN        NaN   NaN  9.63416  NaN   \n",
       "1152227   NaN       NaN        NaN   NaN  9.36360  NaN   \n",
       "1152228   NaN       NaN        NaN   NaN      NaN  NaN   \n",
       "1152229   NaN       NaN        NaN   NaN      NaN  NaN   \n",
       "\n",
       "         FORCE_2020_LITHOFACIES_LITHOLOGY  FORCE_2020_LITHOFACIES_CONFIDENCE  \n",
       "924195                              65000                                2.0  \n",
       "924196                              65000                                2.0  \n",
       "924197                              65000                                2.0  \n",
       "924198                              65000                                2.0  \n",
       "924199                              65000                                2.0  \n",
       "...                                   ...                                ...  \n",
       "1152225                             65030                                2.0  \n",
       "1152226                             65030                                2.0  \n",
       "1152227                             65030                                2.0  \n",
       "1152228                             65030                                2.0  \n",
       "1152229                             65030                                2.0  \n",
       "\n",
       "[132671 rows x 29 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total validation data\n",
    "\n",
    "validation = pd.concat((validation1, validation2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((232134, 29), (99463, 29), (132671, 29))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation.shape, validation1.shape, validation2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous train data shape: (1170511, 29)\n",
      "New train data shape: (938377, 29)\n"
     ]
    }
   ],
   "source": [
    "# dropping validation data from train data\n",
    "\n",
    "new_train = pd.concat([traindata, validation, validation]).drop_duplicates(keep=False)\n",
    "print(f'Previous train data shape: {traindata.shape}')\n",
    "print(f'New train data shape: {new_train.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of previous train data rows: 1170511\n",
      "Validation + new train rows: 1170511\n"
     ]
    }
   ],
   "source": [
    "# QC to ensure there are no data leakage\n",
    "\n",
    "previous_rows = traindata.shape[0]\n",
    "new_train_rows = new_train.shape[0]\n",
    "validation_rows = validation.shape[0]\n",
    "\n",
    "print(f'Number of previous train data rows: {previous_rows}')\n",
    "print(f'Validation + new train rows: {validation_rows+new_train_rows}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65000    577878\n",
       "30000    130927\n",
       "65030    123298\n",
       "70000     44499\n",
       "80000     27737\n",
       "99000     12229\n",
       "70032      8664\n",
       "88000      8213\n",
       "90000      2527\n",
       "74000      1292\n",
       "86000      1010\n",
       "93000       103\n",
       "Name: FORCE_2020_LITHOFACIES_LITHOLOGY, dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to confirm we still have all samples of the labels in the train data set\n",
    "\n",
    "new_train.FORCE_2020_LITHOFACIES_LITHOLOGY.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can proceed to other preparation procedures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logs were selected based on user's desire to use them for training. The confidence logs was dropped as this was absent in the test logs as the ML models need the same set of wells used for training in making predictions on the test wells. Other absent logs and logs with low percentage of values from the combined test data are also dropped. This has previously been visualized in the EDA notebook. A cut off of 30% may be selected as criteria for dropping the logs. Let's take a look at that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of values in test logs:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WELL         100.000000\n",
       "DEPTH_MD     100.000000\n",
       "X_LOC         99.956867\n",
       "Y_LOC         99.956867\n",
       "Z_LOC         99.956867\n",
       "GROUP        100.000000\n",
       "FORMATION     94.828418\n",
       "CALI          95.873116\n",
       "RSHA          28.582603\n",
       "RMED          99.570863\n",
       "RDEP          99.956867\n",
       "RHOB          87.601070\n",
       "GR           100.000000\n",
       "SGR            0.000000\n",
       "NPHI          76.062609\n",
       "PEF           82.978521\n",
       "DTC           99.398330\n",
       "SP            48.708932\n",
       "BS            48.955302\n",
       "ROP           49.943708\n",
       "DTS           31.596801\n",
       "DCAL           9.880397\n",
       "DRHO          81.555130\n",
       "MUDWEIGHT     14.818037\n",
       "RMIC           8.272776\n",
       "ROPA          40.786338\n",
       "RXO           21.820947\n",
       "dtype: float64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Percentage of values in test logs:')\n",
    "100 - testdata.isna().sum()/testdata.shape[0]*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For better and faster processing, the train, validation and test data sets will be concatenated and processed together as we need these data sets to be in the same formats to get good predictions out of the ML model. But let's have it in mind that the RSHA, SGR, DCAL, MUDWEIGHT, RMIC and RXO will be dropped from the wells. Let's proceed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract the data sets indices that will be used for splitting the features and targets into their respective datasets after prepration is complete. We will also be extracting the train target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain = new_train.shape[0]\n",
    "ntest = testdata.shape[0]\n",
    "nvalid1 = validation1.shape[0]\n",
    "nvalid2 = validation2.shape[0]\n",
    "nvalid3 = validation.shape[0]\n",
    "\n",
    "df = pd.concat((new_train, testdata, validation1, validation2, validation)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the combined dataframe for preparation\n",
    "\n",
    "![Picture.png](images/Picture3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1539431, 29)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The procedure below is used to extract data needed for the augmentation procedure to be performed after every other preparation has been done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making a copy of the dataframes\n",
    "\n",
    "train = new_train.copy()\n",
    "test = testdata.copy()\n",
    "valid1 = validation1.copy()\n",
    "valid2 = validation2.copy()\n",
    "valid = validation.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting the data sets well names and depth values needed for augmentation\n",
    "\n",
    "train_well = train.WELL.values\n",
    "train_depth = train.DEPTH_MD.values\n",
    "\n",
    "test_well = test.WELL.values\n",
    "test_depth = test.DEPTH_MD.values\n",
    " \n",
    "valid1_well = valid1.WELL.values\n",
    "valid1_depth = valid1.DEPTH_MD.values\n",
    " \n",
    "valid2_well = valid2.WELL.values\n",
    "valid2_depth = valid2.DEPTH_MD.values\n",
    " \n",
    "valid_well = valid.WELL.values\n",
    "valid_depth = valid.DEPTH_MD.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's extract the data sets labels and prepare them for training and validation performance check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "lithology = train['FORCE_2020_LITHOFACIES_LITHOLOGY']\n",
    "valid1_lithology = valid1['FORCE_2020_LITHOFACIES_LITHOLOGY']\n",
    "valid2_lithology = valid2['FORCE_2020_LITHOFACIES_LITHOLOGY']\n",
    "valid_lithology = valid['FORCE_2020_LITHOFACIES_LITHOLOGY']\n",
    " \n",
    "lithology_numbers = {30000: 0,\n",
    "                 65030: 1,\n",
    "                 65000: 2,\n",
    "                 80000: 3,\n",
    "                 74000: 4,\n",
    "                 70000: 5,\n",
    "                 70032: 6,\n",
    "                 88000: 7,\n",
    "                 86000: 8,\n",
    "                 99000: 9,\n",
    "                 90000: 10,\n",
    "                 93000: 11}\n",
    " \n",
    "lithology = lithology.map(lithology_numbers)\n",
    "valid1_lithology = valid1_lithology.map(lithology_numbers)\n",
    "valid2_lithology = valid2_lithology.map(lithology_numbers)\n",
    "valid_lithology = valid_lithology.map(lithology_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "635461    2\n",
       "635462    2\n",
       "635463    2\n",
       "635464    2\n",
       "635465    2\n",
       "         ..\n",
       "685276    2\n",
       "685277    2\n",
       "685278    2\n",
       "685279    2\n",
       "685280    2\n",
       "Name: FORCE_2020_LITHOFACIES_LITHOLOGY, Length: 99463, dtype: int64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid1_lithology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Encoding\n",
    "\n",
    "The categorical logs/columns in the data set need to be encoded for use by the ML algorithm. From the data visualization, we saw the high dimensionality of the logs (especially the FORMATION log with 69 distinct values), so label encoding will be applied instead of one hot encoding these features to prevent high dimensionality of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1539431, 29)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of dataframe after label encoding columns (1539431, 32)\n"
     ]
    }
   ],
   "source": [
    "df['GROUP_encoded'] = df['GROUP'].astype('category')\n",
    "df['GROUP_encoded'] = df['GROUP_encoded'].cat.codes \n",
    "\n",
    "df['FORMATION_encoded'] = df['FORMATION'].astype('category')\n",
    "df['FORMATION_encoded'] = df['FORMATION_encoded'].cat.codes\n",
    "\n",
    "df['WELL_encoded'] = df['WELL'].astype('category')\n",
    "df['WELL_encoded'] = df['WELL_encoded'].cat.codes\n",
    "\n",
    "print(f'shape of dataframe after label encoding columns {df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WELL</th>\n",
       "      <th>DEPTH_MD</th>\n",
       "      <th>X_LOC</th>\n",
       "      <th>Y_LOC</th>\n",
       "      <th>Z_LOC</th>\n",
       "      <th>GROUP</th>\n",
       "      <th>FORMATION</th>\n",
       "      <th>CALI</th>\n",
       "      <th>RSHA</th>\n",
       "      <th>RMED</th>\n",
       "      <th>...</th>\n",
       "      <th>DRHO</th>\n",
       "      <th>MUDWEIGHT</th>\n",
       "      <th>RMIC</th>\n",
       "      <th>ROPA</th>\n",
       "      <th>RXO</th>\n",
       "      <th>FORCE_2020_LITHOFACIES_LITHOLOGY</th>\n",
       "      <th>FORCE_2020_LITHOFACIES_CONFIDENCE</th>\n",
       "      <th>GROUP_encoded</th>\n",
       "      <th>FORMATION_encoded</th>\n",
       "      <th>WELL_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15/9-13</td>\n",
       "      <td>494.528</td>\n",
       "      <td>437641.96875</td>\n",
       "      <td>6470972.5</td>\n",
       "      <td>-469.501831</td>\n",
       "      <td>NORDLAND GP.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.480835</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.611410</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.574928</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15/9-13</td>\n",
       "      <td>494.680</td>\n",
       "      <td>437641.96875</td>\n",
       "      <td>6470972.5</td>\n",
       "      <td>-469.653809</td>\n",
       "      <td>NORDLAND GP.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.468800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.618070</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.570188</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15/9-13</td>\n",
       "      <td>494.832</td>\n",
       "      <td>437641.96875</td>\n",
       "      <td>6470972.5</td>\n",
       "      <td>-469.805786</td>\n",
       "      <td>NORDLAND GP.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.468800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.626459</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.574245</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15/9-13</td>\n",
       "      <td>494.984</td>\n",
       "      <td>437641.96875</td>\n",
       "      <td>6470972.5</td>\n",
       "      <td>-469.957794</td>\n",
       "      <td>NORDLAND GP.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.459282</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.621594</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.586315</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15/9-13</td>\n",
       "      <td>495.136</td>\n",
       "      <td>437641.96875</td>\n",
       "      <td>6470972.5</td>\n",
       "      <td>-470.109772</td>\n",
       "      <td>NORDLAND GP.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.453100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.602679</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.597914</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      WELL  DEPTH_MD         X_LOC      Y_LOC       Z_LOC         GROUP  \\\n",
       "0  15/9-13   494.528  437641.96875  6470972.5 -469.501831  NORDLAND GP.   \n",
       "1  15/9-13   494.680  437641.96875  6470972.5 -469.653809  NORDLAND GP.   \n",
       "2  15/9-13   494.832  437641.96875  6470972.5 -469.805786  NORDLAND GP.   \n",
       "3  15/9-13   494.984  437641.96875  6470972.5 -469.957794  NORDLAND GP.   \n",
       "4  15/9-13   495.136  437641.96875  6470972.5 -470.109772  NORDLAND GP.   \n",
       "\n",
       "  FORMATION       CALI  RSHA      RMED  ...      DRHO  MUDWEIGHT  RMIC  ROPA  \\\n",
       "0       NaN  19.480835   NaN  1.611410  ... -0.574928        NaN   NaN   NaN   \n",
       "1       NaN  19.468800   NaN  1.618070  ... -0.570188        NaN   NaN   NaN   \n",
       "2       NaN  19.468800   NaN  1.626459  ... -0.574245        NaN   NaN   NaN   \n",
       "3       NaN  19.459282   NaN  1.621594  ... -0.586315        NaN   NaN   NaN   \n",
       "4       NaN  19.453100   NaN  1.602679  ... -0.597914        NaN   NaN   NaN   \n",
       "\n",
       "   RXO  FORCE_2020_LITHOFACIES_LITHOLOGY  FORCE_2020_LITHOFACIES_CONFIDENCE  \\\n",
       "0  NaN                             65000                                1.0   \n",
       "1  NaN                             65000                                1.0   \n",
       "2  NaN                             65000                                1.0   \n",
       "3  NaN                             65000                                1.0   \n",
       "4  NaN                             65000                                1.0   \n",
       "\n",
       "   GROUP_encoded  FORMATION_encoded  WELL_encoded  \n",
       "0              6                 -1             0  \n",
       "1              6                 -1             0  \n",
       "2              6                 -1             0  \n",
       "3              6                 -1             0  \n",
       "4              6                 -1             0  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 5     375804\n",
       " 9     325375\n",
       " 3     162759\n",
       " 7     162020\n",
       " 12    157759\n",
       " 6     152890\n",
       " 2      75038\n",
       " 0      51283\n",
       " 11     33374\n",
       " 4      21425\n",
       " 13     13021\n",
       " 1       3125\n",
       " 8       3075\n",
       "-1       1278\n",
       " 10      1205\n",
       "Name: GROUP_encoded, dtype: int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.GROUP_encoded.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the previous columns after encoding\n",
    "\n",
    "df = df.drop(['WELL', 'GROUP', 'FORMATION'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DEPTH_MD                              0.000000\n",
       "X_LOC                                 0.703961\n",
       "Y_LOC                                 0.703961\n",
       "Z_LOC                                 0.703961\n",
       "CALI                                  7.569550\n",
       "RSHA                                 48.778672\n",
       "RMED                                  2.609081\n",
       "RDEP                                  0.719552\n",
       "RHOB                                 13.044755\n",
       "GR                                    0.000000\n",
       "SGR                                  94.927606\n",
       "NPHI                                 33.245335\n",
       "PEF                                  39.276200\n",
       "DTC                                   5.560626\n",
       "SP                                   27.844509\n",
       "BS                                   42.046899\n",
       "ROP                                  55.059239\n",
       "DTS                                  84.366496\n",
       "DCAL                                 76.267595\n",
       "DRHO                                 15.144362\n",
       "MUDWEIGHT                            74.034887\n",
       "RMIC                                 85.844120\n",
       "ROPA                                 80.243999\n",
       "RXO                                  72.647816\n",
       "FORCE_2020_LITHOFACIES_LITHOLOGY      8.885491\n",
       "FORCE_2020_LITHOFACIES_CONFIDENCE     8.900172\n",
       "GROUP_encoded                         0.000000\n",
       "FORMATION_encoded                     0.000000\n",
       "WELL_encoded                          0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()/df.shape[0]*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling Missing Values\n",
    "\n",
    "Some fractions of missing values still exist in present logs, how do we resolve that? While we can use a mean of values in a window to solve this, backward or forward fill, we could also decide to fill up all missing values with a distinct value different from other values. This way, the ML algorithm used (in this case a gradient tree algorithm) can differentiate this better. From validation, this improved result better. -9999 is used, and since this is a classification problem as opposed to a regression where we predict actual lithology values, outlier effect is not observed in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(-9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DEPTH_MD                             0.0\n",
       "X_LOC                                0.0\n",
       "Y_LOC                                0.0\n",
       "Z_LOC                                0.0\n",
       "CALI                                 0.0\n",
       "RSHA                                 0.0\n",
       "RMED                                 0.0\n",
       "RDEP                                 0.0\n",
       "RHOB                                 0.0\n",
       "GR                                   0.0\n",
       "SGR                                  0.0\n",
       "NPHI                                 0.0\n",
       "PEF                                  0.0\n",
       "DTC                                  0.0\n",
       "SP                                   0.0\n",
       "BS                                   0.0\n",
       "ROP                                  0.0\n",
       "DTS                                  0.0\n",
       "DCAL                                 0.0\n",
       "DRHO                                 0.0\n",
       "MUDWEIGHT                            0.0\n",
       "RMIC                                 0.0\n",
       "ROPA                                 0.0\n",
       "RXO                                  0.0\n",
       "FORCE_2020_LITHOFACIES_LITHOLOGY     0.0\n",
       "FORCE_2020_LITHOFACIES_CONFIDENCE    0.0\n",
       "GROUP_encoded                        0.0\n",
       "FORMATION_encoded                    0.0\n",
       "WELL_encoded                         0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()/df.shape[0]*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've completed the majority of the preparation, let's split back our concatenated dataframe into their validation sets, train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1539431, 29)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = df.copy()   #making a copy of the preparaed dataframe to work with\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember the shape of the concatenated dataframe;\n",
    "\n",
    "![Picture.png](images/Picture3.png)\n",
    "\n",
    "using the data sets indices will be used for slicing out their corresponding features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data[:ntrain].copy()\n",
    "train.drop(['FORCE_2020_LITHOFACIES_LITHOLOGY'], axis=1, inplace=True)\n",
    "        \n",
    "test = data[ntrain:(ntest+ntrain)].copy()\n",
    "test.drop(['FORCE_2020_LITHOFACIES_LITHOLOGY'], axis=1, inplace=True)\n",
    "test = test.reset_index(drop=True)\n",
    "\n",
    "valid1 = data[(ntest+ntrain):(ntest+ntrain+nvalid1)].copy()\n",
    "valid1.drop(['FORCE_2020_LITHOFACIES_LITHOLOGY'], axis=1, inplace=True)\n",
    "valid1 = valid1.reset_index(drop=True)\n",
    "\n",
    "valid2 = data[(ntest+ntrain+nvalid1):(ntest+ntrain+nvalid1+nvalid2)].copy()\n",
    "valid2.drop(['FORCE_2020_LITHOFACIES_LITHOLOGY'], axis=1, inplace=True)\n",
    "valid2 = valid2.reset_index(drop=True)\n",
    "\n",
    "valid = data[(ntest+ntrain+nvalid1+nvalid2):].copy()\n",
    "valid.drop(['FORCE_2020_LITHOFACIES_LITHOLOGY'], axis=1, inplace=True)\n",
    "valid = valid.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((938377, 28), (136786, 28), (99463, 28), (132671, 28), (232134, 28))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking shapes of sliced data sets for QC\n",
    "\n",
    "train.shape, test.shape, valid1.shape, valid2.shape, valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "\n",
    "The data augmentation technique is extracted from the ISPL team code for the 2016 SEG ML competition : https://github.com/seg/2016-ml-contest/tree/master/ispl . The technique was based on the assumption that \"facies do not abrutly change from a given depth layer to the next one\". This was implemented by aggregating features at neighbouring depths and computing the feature spatial gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature windows concatenation function\n",
    "def augment_features_window(X, N_neig):\n",
    "    \n",
    "    # Parameters\n",
    "    N_row = X.shape[0]\n",
    "    N_feat = X.shape[1]\n",
    " \n",
    "    # Zero padding\n",
    "    X = np.vstack((np.zeros((N_neig, N_feat)), X, (np.zeros((N_neig, N_feat)))))\n",
    " \n",
    "    # Loop over windows\n",
    "    X_aug = np.zeros((N_row, N_feat*(2*N_neig+1)))\n",
    "    for r in np.arange(N_row)+N_neig:\n",
    "        this_row = []\n",
    "        for c in np.arange(-N_neig,N_neig+1):\n",
    "            this_row = np.hstack((this_row, X[r+c]))\n",
    "        X_aug[r-N_neig] = this_row\n",
    " \n",
    "    return X_aug\n",
    " \n",
    "# Feature gradient computation function\n",
    "def augment_features_gradient(X, depth):\n",
    "    \n",
    "    # Compute features gradient\n",
    "    d_diff = np.diff(depth).reshape((-1, 1))\n",
    "    d_diff[d_diff==0] = 0.001\n",
    "    X_diff = np.diff(X, axis=0)\n",
    "    X_grad = X_diff / d_diff\n",
    "        \n",
    "    # Compensate for last missing value\n",
    "    X_grad = np.concatenate((X_grad, np.zeros((1, X_grad.shape[1]))))\n",
    "    \n",
    "    return X_grad\n",
    " \n",
    "# Feature augmentation function\n",
    "def augment_features(X, well, depth, N_neig=1):\n",
    "    \n",
    "    # Augment features\n",
    "    X_aug = np.zeros((X.shape[0], X.shape[1]*(N_neig*2+2)))\n",
    "    for w in np.unique(well):\n",
    "        w_idx = np.where(well == w)[0]\n",
    "        X_aug_win = augment_features_window(X[w_idx, :], N_neig)\n",
    "        X_aug_grad = augment_features_gradient(X[w_idx, :], depth[w_idx])\n",
    "        X_aug[w_idx, :] = np.concatenate((X_aug_win, X_aug_grad), axis=1)\n",
    "    \n",
    "    return X_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of datasets before augmentation ((938377, 28), (136786, 28), (99463, 28), (132671, 28), (232134, 28))\n",
      "Shape of datasets after augmentation ((938377, 112), (136786, 112), (99463, 112), (132671, 112), (232134, 112))\n"
     ]
    }
   ],
   "source": [
    "print(f'Shape of datasets before augmentation {train.shape, test.shape, valid1.shape, valid2.shape, valid.shape}')\n",
    "\n",
    "aug_train = augment_features(train.values, train_well, train_depth)\n",
    "aug_test = augment_features(test.values, test_well, test_depth)\n",
    "aug_valid1 = augment_features(valid1.values, valid1_well, valid1_depth)\n",
    "aug_valid2 = augment_features(valid2.values, valid2_well, valid2_depth)\n",
    "aug_valid = augment_features(valid.values, valid_well, valid_depth)\n",
    "\n",
    "print(f'Shape of datasets after augmentation {aug_train.shape, aug_test.shape, aug_valid1.shape, aug_valid2.shape, aug_valid.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.031179</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>494.528</td>\n",
       "      <td>437641.96875</td>\n",
       "      <td>6470972.5</td>\n",
       "      <td>-469.501831</td>\n",
       "      <td>19.480835</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>1.611410</td>\n",
       "      <td>1.798681</td>\n",
       "      <td>1.884186</td>\n",
       "      <td>80.200851</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.026689</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>494.680</td>\n",
       "      <td>437641.96875</td>\n",
       "      <td>6470972.5</td>\n",
       "      <td>-469.653809</td>\n",
       "      <td>19.468800</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>1.618070</td>\n",
       "      <td>1.795641</td>\n",
       "      <td>1.889794</td>\n",
       "      <td>79.262886</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.079409</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>494.832</td>\n",
       "      <td>437641.96875</td>\n",
       "      <td>6470972.5</td>\n",
       "      <td>-469.805786</td>\n",
       "      <td>19.468800</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>1.626459</td>\n",
       "      <td>1.800733</td>\n",
       "      <td>1.896523</td>\n",
       "      <td>74.821999</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.076305</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>494.984</td>\n",
       "      <td>437641.96875</td>\n",
       "      <td>6470972.5</td>\n",
       "      <td>-469.957794</td>\n",
       "      <td>19.459282</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>1.621594</td>\n",
       "      <td>1.801517</td>\n",
       "      <td>1.891913</td>\n",
       "      <td>72.878922</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.024252</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>495.136</td>\n",
       "      <td>437641.96875</td>\n",
       "      <td>6470972.5</td>\n",
       "      <td>-470.109772</td>\n",
       "      <td>19.453100</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>1.602679</td>\n",
       "      <td>1.795299</td>\n",
       "      <td>1.880034</td>\n",
       "      <td>71.729141</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021260</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>495.288</td>\n",
       "      <td>437641.96875</td>\n",
       "      <td>6470972.5</td>\n",
       "      <td>-470.261780</td>\n",
       "      <td>19.453100</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>1.585567</td>\n",
       "      <td>1.804719</td>\n",
       "      <td>1.879687</td>\n",
       "      <td>72.014420</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.024150</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>495.440</td>\n",
       "      <td>437641.96875</td>\n",
       "      <td>6470972.5</td>\n",
       "      <td>-470.413788</td>\n",
       "      <td>19.462496</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>1.576569</td>\n",
       "      <td>1.805498</td>\n",
       "      <td>1.878731</td>\n",
       "      <td>72.588089</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.081083</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>495.592</td>\n",
       "      <td>437641.96875</td>\n",
       "      <td>6470972.5</td>\n",
       "      <td>-470.565796</td>\n",
       "      <td>19.468800</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>1.587011</td>\n",
       "      <td>1.808367</td>\n",
       "      <td>1.867837</td>\n",
       "      <td>71.283051</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.049004</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>495.744</td>\n",
       "      <td>437641.96875</td>\n",
       "      <td>6470972.5</td>\n",
       "      <td>-470.717773</td>\n",
       "      <td>19.468800</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>1.613674</td>\n",
       "      <td>1.815813</td>\n",
       "      <td>1.847233</td>\n",
       "      <td>69.721436</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.065673</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 112 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0             1          2           3          4       5         6    \\\n",
       "0    0.000       0.00000        0.0    0.000000   0.000000     0.0  0.000000   \n",
       "1  494.528  437641.96875  6470972.5 -469.501831  19.480835 -9999.0  1.611410   \n",
       "2  494.680  437641.96875  6470972.5 -469.653809  19.468800 -9999.0  1.618070   \n",
       "3  494.832  437641.96875  6470972.5 -469.805786  19.468800 -9999.0  1.626459   \n",
       "4  494.984  437641.96875  6470972.5 -469.957794  19.459282 -9999.0  1.621594   \n",
       "5  495.136  437641.96875  6470972.5 -470.109772  19.453100 -9999.0  1.602679   \n",
       "6  495.288  437641.96875  6470972.5 -470.261780  19.453100 -9999.0  1.585567   \n",
       "7  495.440  437641.96875  6470972.5 -470.413788  19.462496 -9999.0  1.576569   \n",
       "8  495.592  437641.96875  6470972.5 -470.565796  19.468800 -9999.0  1.587011   \n",
       "9  495.744  437641.96875  6470972.5 -470.717773  19.468800 -9999.0  1.613674   \n",
       "\n",
       "        7         8          9    ...  102       103  104  105  106  107  108  \\\n",
       "0  0.000000  0.000000   0.000000  ...  0.0  0.031179  0.0  0.0  0.0  0.0  0.0   \n",
       "1  1.798681  1.884186  80.200851  ...  0.0 -0.026689  0.0  0.0  0.0  0.0  0.0   \n",
       "2  1.795641  1.889794  79.262886  ...  0.0 -0.079409  0.0  0.0  0.0  0.0  0.0   \n",
       "3  1.800733  1.896523  74.821999  ...  0.0 -0.076305  0.0  0.0  0.0  0.0  0.0   \n",
       "4  1.801517  1.891913  72.878922  ...  0.0 -0.024252  0.0  0.0  0.0  0.0  0.0   \n",
       "5  1.795299  1.880034  71.729141  ...  0.0  0.021260  0.0  0.0  0.0  0.0  0.0   \n",
       "6  1.804719  1.879687  72.014420  ...  0.0 -0.024150  0.0  0.0  0.0  0.0  0.0   \n",
       "7  1.805498  1.878731  72.588089  ...  0.0 -0.081083  0.0  0.0  0.0  0.0  0.0   \n",
       "8  1.808367  1.867837  71.283051  ...  0.0 -0.049004  0.0  0.0  0.0  0.0  0.0   \n",
       "9  1.815813  1.847233  69.721436  ...  0.0  0.065673  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "   109  110  111  \n",
       "0  0.0  0.0  0.0  \n",
       "1  0.0  0.0  0.0  \n",
       "2  0.0  0.0  0.0  \n",
       "3  0.0  0.0  0.0  \n",
       "4  0.0  0.0  0.0  \n",
       "5  0.0  0.0  0.0  \n",
       "6  0.0  0.0  0.0  \n",
       "7  0.0  0.0  0.0  \n",
       "8  0.0  0.0  0.0  \n",
       "9  0.0  0.0  0.0  \n",
       "\n",
       "[10 rows x 112 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(aug_train).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DEPTH_MD</th>\n",
       "      <th>X_LOC</th>\n",
       "      <th>Y_LOC</th>\n",
       "      <th>Z_LOC</th>\n",
       "      <th>CALI</th>\n",
       "      <th>RSHA</th>\n",
       "      <th>RMED</th>\n",
       "      <th>RDEP</th>\n",
       "      <th>RHOB</th>\n",
       "      <th>GR</th>\n",
       "      <th>...</th>\n",
       "      <th>DCAL</th>\n",
       "      <th>DRHO</th>\n",
       "      <th>MUDWEIGHT</th>\n",
       "      <th>RMIC</th>\n",
       "      <th>ROPA</th>\n",
       "      <th>RXO</th>\n",
       "      <th>FORCE_2020_LITHOFACIES_CONFIDENCE</th>\n",
       "      <th>GROUP_encoded</th>\n",
       "      <th>FORMATION_encoded</th>\n",
       "      <th>WELL_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>494.528</td>\n",
       "      <td>437641.96875</td>\n",
       "      <td>6470972.5</td>\n",
       "      <td>-469.501831</td>\n",
       "      <td>19.480835</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>1.611410</td>\n",
       "      <td>1.798681</td>\n",
       "      <td>1.884186</td>\n",
       "      <td>80.200851</td>\n",
       "      <td>...</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-0.574928</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>494.680</td>\n",
       "      <td>437641.96875</td>\n",
       "      <td>6470972.5</td>\n",
       "      <td>-469.653809</td>\n",
       "      <td>19.468800</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>1.618070</td>\n",
       "      <td>1.795641</td>\n",
       "      <td>1.889794</td>\n",
       "      <td>79.262886</td>\n",
       "      <td>...</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-0.570188</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>494.832</td>\n",
       "      <td>437641.96875</td>\n",
       "      <td>6470972.5</td>\n",
       "      <td>-469.805786</td>\n",
       "      <td>19.468800</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>1.626459</td>\n",
       "      <td>1.800733</td>\n",
       "      <td>1.896523</td>\n",
       "      <td>74.821999</td>\n",
       "      <td>...</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-0.574245</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>494.984</td>\n",
       "      <td>437641.96875</td>\n",
       "      <td>6470972.5</td>\n",
       "      <td>-469.957794</td>\n",
       "      <td>19.459282</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>1.621594</td>\n",
       "      <td>1.801517</td>\n",
       "      <td>1.891913</td>\n",
       "      <td>72.878922</td>\n",
       "      <td>...</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-0.586315</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>495.136</td>\n",
       "      <td>437641.96875</td>\n",
       "      <td>6470972.5</td>\n",
       "      <td>-470.109772</td>\n",
       "      <td>19.453100</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>1.602679</td>\n",
       "      <td>1.795299</td>\n",
       "      <td>1.880034</td>\n",
       "      <td>71.729141</td>\n",
       "      <td>...</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-0.597914</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>495.288</td>\n",
       "      <td>437641.96875</td>\n",
       "      <td>6470972.5</td>\n",
       "      <td>-470.261780</td>\n",
       "      <td>19.453100</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>1.585567</td>\n",
       "      <td>1.804719</td>\n",
       "      <td>1.879687</td>\n",
       "      <td>72.014420</td>\n",
       "      <td>...</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-0.601600</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>495.440</td>\n",
       "      <td>437641.96875</td>\n",
       "      <td>6470972.5</td>\n",
       "      <td>-470.413788</td>\n",
       "      <td>19.462496</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>1.576569</td>\n",
       "      <td>1.805498</td>\n",
       "      <td>1.878731</td>\n",
       "      <td>72.588089</td>\n",
       "      <td>...</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-0.598369</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>495.592</td>\n",
       "      <td>437641.96875</td>\n",
       "      <td>6470972.5</td>\n",
       "      <td>-470.565796</td>\n",
       "      <td>19.468800</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>1.587011</td>\n",
       "      <td>1.808367</td>\n",
       "      <td>1.867837</td>\n",
       "      <td>71.283051</td>\n",
       "      <td>...</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-0.602039</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>495.744</td>\n",
       "      <td>437641.96875</td>\n",
       "      <td>6470972.5</td>\n",
       "      <td>-470.717773</td>\n",
       "      <td>19.468800</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>1.613674</td>\n",
       "      <td>1.815813</td>\n",
       "      <td>1.847233</td>\n",
       "      <td>69.721436</td>\n",
       "      <td>...</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-0.614364</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>495.896</td>\n",
       "      <td>437641.96875</td>\n",
       "      <td>6470972.5</td>\n",
       "      <td>-470.869781</td>\n",
       "      <td>19.468800</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>1.634622</td>\n",
       "      <td>1.813916</td>\n",
       "      <td>1.836309</td>\n",
       "      <td>66.677727</td>\n",
       "      <td>...</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-0.621813</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   DEPTH_MD         X_LOC      Y_LOC       Z_LOC       CALI    RSHA      RMED  \\\n",
       "0   494.528  437641.96875  6470972.5 -469.501831  19.480835 -9999.0  1.611410   \n",
       "1   494.680  437641.96875  6470972.5 -469.653809  19.468800 -9999.0  1.618070   \n",
       "2   494.832  437641.96875  6470972.5 -469.805786  19.468800 -9999.0  1.626459   \n",
       "3   494.984  437641.96875  6470972.5 -469.957794  19.459282 -9999.0  1.621594   \n",
       "4   495.136  437641.96875  6470972.5 -470.109772  19.453100 -9999.0  1.602679   \n",
       "5   495.288  437641.96875  6470972.5 -470.261780  19.453100 -9999.0  1.585567   \n",
       "6   495.440  437641.96875  6470972.5 -470.413788  19.462496 -9999.0  1.576569   \n",
       "7   495.592  437641.96875  6470972.5 -470.565796  19.468800 -9999.0  1.587011   \n",
       "8   495.744  437641.96875  6470972.5 -470.717773  19.468800 -9999.0  1.613674   \n",
       "9   495.896  437641.96875  6470972.5 -470.869781  19.468800 -9999.0  1.634622   \n",
       "\n",
       "       RDEP      RHOB         GR  ...    DCAL      DRHO  MUDWEIGHT    RMIC  \\\n",
       "0  1.798681  1.884186  80.200851  ... -9999.0 -0.574928    -9999.0 -9999.0   \n",
       "1  1.795641  1.889794  79.262886  ... -9999.0 -0.570188    -9999.0 -9999.0   \n",
       "2  1.800733  1.896523  74.821999  ... -9999.0 -0.574245    -9999.0 -9999.0   \n",
       "3  1.801517  1.891913  72.878922  ... -9999.0 -0.586315    -9999.0 -9999.0   \n",
       "4  1.795299  1.880034  71.729141  ... -9999.0 -0.597914    -9999.0 -9999.0   \n",
       "5  1.804719  1.879687  72.014420  ... -9999.0 -0.601600    -9999.0 -9999.0   \n",
       "6  1.805498  1.878731  72.588089  ... -9999.0 -0.598369    -9999.0 -9999.0   \n",
       "7  1.808367  1.867837  71.283051  ... -9999.0 -0.602039    -9999.0 -9999.0   \n",
       "8  1.815813  1.847233  69.721436  ... -9999.0 -0.614364    -9999.0 -9999.0   \n",
       "9  1.813916  1.836309  66.677727  ... -9999.0 -0.621813    -9999.0 -9999.0   \n",
       "\n",
       "     ROPA     RXO  FORCE_2020_LITHOFACIES_CONFIDENCE  GROUP_encoded  \\\n",
       "0 -9999.0 -9999.0                                1.0              6   \n",
       "1 -9999.0 -9999.0                                1.0              6   \n",
       "2 -9999.0 -9999.0                                1.0              6   \n",
       "3 -9999.0 -9999.0                                1.0              6   \n",
       "4 -9999.0 -9999.0                                1.0              6   \n",
       "5 -9999.0 -9999.0                                1.0              6   \n",
       "6 -9999.0 -9999.0                                1.0              6   \n",
       "7 -9999.0 -9999.0                                1.0              6   \n",
       "8 -9999.0 -9999.0                                1.0              6   \n",
       "9 -9999.0 -9999.0                                1.0              6   \n",
       "\n",
       "   FORMATION_encoded  WELL_encoded  \n",
       "0                 -1             0  \n",
       "1                 -1             0  \n",
       "2                 -1             0  \n",
       "3                 -1             0  \n",
       "4                 -1             0  \n",
       "5                 -1             0  \n",
       "6                 -1             0  \n",
       "7                 -1             0  \n",
       "8                 -1             0  \n",
       "9                 -1             0  \n",
       "\n",
       "[10 rows x 28 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "\n",
    "The choice of algorithm for this tutorial workflow is xgboost. Why? Performance on previously done validation was better, and also at a faster compute speed than catboost. Random forest is also a great algorithm to try. Let's implement our xgboost tree. This will be done in a 10 fold cross validation technique. This is done to get a better performance and a confident result that is not due to randomness. We will be using StratifiedKFold function from sklearn. Let's look at that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_evaluation(pred, true):\n",
    "\n",
    "  '''\n",
    "\n",
    "  function to show model performance and evaluation\n",
    "  args:\n",
    "    pred: predicted value(a list)\n",
    "    true: actual values (a list)\n",
    "\n",
    "  prints the custom metric performance, accuracy and F1 score of predictions\n",
    "\n",
    "  '''\n",
    "\n",
    "  print(f'Default score: {score(true.values, pred)}')\n",
    "  print(f'Accuracy is: {accuracy_score(true, pred)}')\n",
    "  print(f'F1 is: {f1_score(pred, true.values, average=\"weighted\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing the xgboost model\n",
    "\n",
    "model = xgb.XGBClassifier(n_estimators=100, max_depth=10, booster='gbtree',\n",
    "                          objective='softprob', learning_rate=0.1, random_state=0,\n",
    "                          subsample=0.9, colsample_bytree=0.9, tree_method='gpu_hist',\n",
    "                          eval_metric='mlogloss', reg_lambda=1500, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 10\n",
    "\n",
    "kf = StratifiedKFold(n_splits=split, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing the prediction probabilities arrays\n",
    "\n",
    "test_pred = np.zeros((len(test), 12))\n",
    "valid1_pred = np.zeros((len(valid1), 12))\n",
    "valid2_pred = np.zeros((len(valid2), 12))\n",
    "valid_pred = np.zeros((len(valid), 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(847170,) (94131,)\n",
      "[     0      1      2 ... 941298 941299 941300] [     3     12     17 ... 941254 941261 941297]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:2.16357\n",
      "Default score: -0.9455169215021432\n",
      "Accuracy is: 0.6839187085492504\n",
      "F1 is: 0.6969638976491602\n",
      "None\n",
      "Default score: -0.8728242084693497\n",
      "Accuracy is: 0.6792874812212671\n",
      "F1 is: 0.7068894813860023\n",
      "None\n",
      "Default score: -0.9174397932027398\n",
      "Accuracy is: 0.6821299245233629\n",
      "F1 is: 0.7001680772519585\n",
      "None\n",
      "----------------------- FOLD 1 ---------------------\n",
      "(847171,) (94130,)\n",
      "[     0      1      2 ... 941298 941299 941300] [     6     20     51 ... 941242 941258 941293]\n",
      "[0]\tvalidation_0-mlogloss:2.16257\n",
      "Default score: -0.9691825005864415\n",
      "Accuracy is: 0.6774856232984312\n",
      "F1 is: 0.6894236403360214\n",
      "None\n",
      "Default score: -0.8684528583208142\n",
      "Accuracy is: 0.6769041352746495\n",
      "F1 is: 0.7084191911073783\n",
      "None\n",
      "Default score: -0.9302762750316304\n",
      "Accuracy is: 0.6772610270058025\n",
      "F1 is: 0.696218032206216\n",
      "None\n",
      "----------------------- FOLD 2 ---------------------\n",
      "(847171,) (94130,)\n",
      "[     0      1      2 ... 941297 941298 941300] [     4     13     18 ... 941290 941294 941299]\n",
      "[0]\tvalidation_0-mlogloss:2.16286\n",
      "Default score: -1.03928269322358\n",
      "Accuracy is: 0.6589682895101614\n",
      "F1 is: 0.6672680680172076\n",
      "None\n",
      "Default score: -0.8988334594661757\n",
      "Accuracy is: 0.6830714664919633\n",
      "F1 is: 0.7289019711552439\n",
      "None\n",
      "Default score: -0.9850350115614502\n",
      "Accuracy is: 0.6682779983421316\n",
      "F1 is: 0.6910126811847215\n",
      "None\n",
      "----------------------- FOLD 3 ---------------------\n",
      "(847171,) (94130,)\n",
      "[     0      1      2 ... 941298 941299 941300] [    24     27     34 ... 941266 941275 941286]\n",
      "[0]\tvalidation_0-mlogloss:2.16301\n",
      "Default score: -0.9485975163315065\n",
      "Accuracy is: 0.6827245004584906\n",
      "F1 is: 0.6962427853308006\n",
      "None\n",
      "Default score: -0.9039785498864804\n",
      "Accuracy is: 0.6878607493420383\n",
      "F1 is: 0.7311027176925674\n",
      "None\n",
      "Default score: -0.9313637057719995\n",
      "Accuracy is: 0.6847083460581999\n",
      "F1 is: 0.70889723211809\n",
      "None\n",
      "----------------------- FOLD 4 ---------------------\n",
      "(847171,) (94130,)\n",
      "[     0      1      2 ... 941298 941299 941300] [     5     16     28 ... 941267 941274 941291]\n",
      "[0]\tvalidation_0-mlogloss:2.16309\n",
      "Default score: -0.9389096809047548\n",
      "Accuracy is: 0.6854683357146412\n",
      "F1 is: 0.696823782421718\n",
      "None\n",
      "Default score: -0.9664778439190792\n",
      "Accuracy is: 0.658831369802668\n",
      "F1 is: 0.7104931364788903\n",
      "None\n",
      "Default score: -0.9495577199947647\n",
      "Accuracy is: 0.6751799659700711\n",
      "F1 is: 0.7014547859000553\n",
      "None\n",
      "----------------------- FOLD 5 ---------------------\n",
      "(847171,) (94130,)\n",
      "[     0      1      2 ... 941298 941299 941300] [     7     22     36 ... 941280 941284 941289]\n",
      "[0]\tvalidation_0-mlogloss:2.16362\n",
      "Default score: -0.9450664278250485\n",
      "Accuracy is: 0.6846579802244827\n",
      "F1 is: 0.6970626815819945\n",
      "None\n",
      "Default score: -0.869568286814788\n",
      "Accuracy is: 0.6805864612395658\n",
      "F1 is: 0.7117328915041178\n",
      "None\n",
      "Default score: -0.9159057196457397\n",
      "Accuracy is: 0.6830853802190131\n",
      "F1 is: 0.7025762236033966\n",
      "None\n",
      "----------------------- FOLD 6 ---------------------\n",
      "(847171,) (94130,)\n",
      "[     1      2      3 ... 941298 941299 941300] [     0     15     21 ... 941276 941281 941287]\n",
      "[0]\tvalidation_0-mlogloss:2.16418\n",
      "Default score: -0.9530775737672289\n",
      "Accuracy is: 0.6809900553742918\n",
      "F1 is: 0.6934704850511356\n",
      "None\n",
      "Default score: -0.850010165930578\n",
      "Accuracy is: 0.6870248839389592\n",
      "F1 is: 0.7163728241067662\n",
      "None\n",
      "Default score: -0.9132684001570612\n",
      "Accuracy is: 0.6833209720343789\n",
      "F1 is: 0.7017528033381235\n",
      "None\n",
      "----------------------- FOLD 7 ---------------------\n",
      "(847171,) (94130,)\n",
      "[     0      2      3 ... 941297 941298 941299] [     1      8      9 ... 941292 941296 941300]\n",
      "[0]\tvalidation_0-mlogloss:2.16264\n",
      "Default score: -0.9553993488722553\n",
      "Accuracy is: 0.6820705293611697\n",
      "F1 is: 0.695976507441613\n",
      "None\n",
      "Default score: -0.9472218770826039\n",
      "Accuracy is: 0.6420124024353051\n",
      "F1 is: 0.6867712225543027\n",
      "None\n",
      "Default score: -0.9522408490030976\n",
      "Accuracy is: 0.6665983159548012\n",
      "F1 is: 0.6923255705068011\n",
      "None\n",
      "----------------------- FOLD 8 ---------------------\n",
      "(847171,) (94130,)\n",
      "[     0      1      3 ... 941297 941299 941300] [     2     10     11 ... 941269 941288 941298]\n",
      "[0]\tvalidation_0-mlogloss:2.16313\n",
      "Default score: -1.0310298623106504\n",
      "Accuracy is: 0.6604183993346555\n",
      "F1 is: 0.6687665862705917\n",
      "None\n",
      "Default score: -0.9216757407009973\n",
      "Accuracy is: 0.6504501248150365\n",
      "F1 is: 0.6984325622996765\n",
      "None\n",
      "Default score: -0.9887924828759652\n",
      "Accuracy is: 0.6565682125561711\n",
      "F1 is: 0.6791149602813025\n",
      "None\n",
      "----------------------- FOLD 9 ---------------------\n",
      "(847171,) (94130,)\n",
      "[     0      1      2 ... 941298 941299 941300] [    35     55     67 ... 941283 941285 941295]\n",
      "[0]\tvalidation_0-mlogloss:2.16300\n",
      "Default score: -0.9480146290491118\n",
      "Accuracy is: 0.6829377519032691\n",
      "F1 is: 0.6943052316125679\n",
      "None\n",
      "Default score: -0.9411449096926501\n",
      "Accuracy is: 0.6719341247698546\n",
      "F1 is: 0.7116880985844654\n",
      "None\n",
      "Default score: -0.9453612407835609\n",
      "Accuracy is: 0.6786876663321845\n",
      "F1 is: 0.7004643507485608\n",
      "None\n",
      "----------------------- FOLD 10 ---------------------\n"
     ]
    }
   ],
   "source": [
    "#implementing the CV Loop\n",
    "\n",
    "i = 1\n",
    "for (train_index, test_index) in kf.split(train, lithology):\n",
    "    print(train_index.shape, test_index.shape)\n",
    "    print(train_index, test_index)\n",
    "    X_train,X_test = train.iloc[train_index], train.iloc[test_index]\n",
    "    Y_train,Y_test = lithology.iloc[train_index], lithology.iloc[test_index]\n",
    "    \n",
    "        \n",
    "    model.fit(X_train, Y_train, early_stopping_rounds=100, eval_set=[(X_test, Y_test)], verbose=1)\n",
    "    \n",
    "    prediction1 = model.predict(valid1)\n",
    "    prediction2 = model.predict(valid2)\n",
    "    prediction = model.predict(valid)\n",
    "    \n",
    "    print(show_evaluation(prediction1, valid1_lithology))\n",
    "    print(show_evaluation(prediction2, valid2_lithology))\n",
    "    print(show_evaluation(prediction, valid_lithology))\n",
    " \n",
    "    print(f'----------------------- FOLD {i} ---------------------')\n",
    "    i+=1\n",
    "    \n",
    "    valid1_pred += model.predict_proba(valid1)\n",
    "    valid2_pred += model.predict_proba(valid2)\n",
    "    valid_pred += model.predict_proba(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the probabilities average and converting the numpy array to a dataframe\n",
    "\n",
    "valid1_pred = pd.DataFrame(valid1_pred/split)\n",
    "valid2_pred = pd.DataFrame(valid2_pred/split)\n",
    "valid_pred = pd.DataFrame(valid_pred/split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting the index position with the highest probability as the lithology classp\n",
    "\n",
    "valid1_pred = valid1_pred.idxmax(axis=1)\n",
    "valid2_pred = valid2_pred.idxmax(axis=1)\n",
    "valid_pred = valid_pred.idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the final predictions from the CV and max probability indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default score: -0.9571631160301111\n",
      "Accuracy is: 0.6801939166471186\n",
      "F1 is: 0.6918461589943973\n"
     ]
    }
   ],
   "source": [
    "show_evaluation(valid1_pred, valid1_lithology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default score: -0.8554701178118399\n",
      "Accuracy is: 0.6841784233771221\n",
      "F1 is: 0.7175537752878195\n"
     ]
    }
   ],
   "source": [
    "show_evaluation(valid2_pred, valid2_lithology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default score: -0.9178847999650975\n",
      "Accuracy is: 0.6817329086863575\n",
      "F1 is: 0.7019928305401797\n"
     ]
    }
   ],
   "source": [
    "show_evaluation(valid_pred, valid_lithology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
